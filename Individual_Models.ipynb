{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install streamlit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xlsxwriter\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "# conda install -c conda-forge scikit-learn-extra\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from operator import itemgetter\n",
    "\n",
    "# Principal Components Analysis\n",
    "# from scipy import stats\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# Classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "df = pd.read_csv('dgn_raw_data.csv')\n",
    "\n",
    "# Add very small random number to Rating\n",
    "df['target']=df['Rating'].apply(lambda x: x+random.random()/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions for Each UID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique IDs\n",
    "ids = df.UID.unique()\n",
    "\n",
    "# Run linear regressions for each UID\n",
    "op = pd.DataFrame\n",
    "intercept = []\n",
    "coefficients=[]\n",
    "UID = []\n",
    "for p in ids:\n",
    "    df_i = df[df.UID == p]              # Create dataframe for current user id\n",
    "    X = df_i.filter(regex='^[a-zA-Z][0-9]')  # df input variables only\n",
    "    y = df_i['target']                  # Series of target variable\n",
    "    reg = LinearRegression().fit(X, y)  # Fit linear regression\n",
    "    reg.score(X, y)                     # Score regression model\n",
    "    unique_id=df_i['UID'].unique()      # Saves current user id\n",
    "    const = reg.intercept_              # Save intercept of the regression model\n",
    "    coef = reg.coef_                    # Coefficients of regression model\n",
    "    UID.append(unique_id)               # Append current user id\n",
    "    intercept.append(const)             # Append current intercept\n",
    "    coefficients.append(coef)           # Append current regression coefficients\n",
    "\n",
    "# Convert newly created lists into dataframes\n",
    "intercep_new = pd.DataFrame(intercept)\n",
    "coefficients_new = pd.DataFrame(coefficients)\n",
    "UID_new = pd.DataFrame(UID)\n",
    "\n",
    "# Get columns names\n",
    "colNames = df.drop(['Rating', 'target',], axis=1).columns\n",
    "colNames = colNames.insert(1, 'Const')\n",
    "colNames\n",
    "\n",
    "# Concatenate the new dataframes and add column names\n",
    "op = pd.concat([UID_new,intercep_new, coefficients_new], axis=1)\n",
    "op.columns = colNames\n",
    "\n",
    "# Save only regression coefficients for clustering\n",
    "scores = op.drop(['UID','Const'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster on Regression Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('pandas_multiple.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Create dataframe for storing all cluster/variable combo averages and stdevs\n",
    "cls_averages_all = pd.DataFrame()\n",
    "\n",
    "# All maps for all cluster solutions\n",
    "all_maps = []\n",
    "\n",
    "# Column of last cluster solution\n",
    "first_var = 18\n",
    "last_var = op.shape[1]\n",
    "\n",
    "# Maximum number of clusters\n",
    "max_clusters = 6\n",
    "\n",
    "for n in range(2, max_clusters+1):\n",
    "    \n",
    "    sw = []\n",
    "    \n",
    "    # Create clustering objects\n",
    "    cls1 = KMeans(n_clusters=n, random_state=0)\n",
    "    cls2 = KMedoids(n_clusters=n, random_state=0)\n",
    "    cls3 = AgglomerativeClustering(n_clusters=n,\n",
    "                                   affinity='euclidean',\n",
    "                                   linkage='ward')\n",
    "        # Agglomerative clustering: if linkage=ward, affinity must be Euclidean\n",
    "    cls_algs = [['kMeans', cls1],\n",
    "                ['kMedoids', cls2],\n",
    "                ['Hierarchical', cls3]]\n",
    "    \n",
    "    # Fit and score clustering solutions for i clusters w/ each algorithm\n",
    "    for cls in cls_algs:\n",
    "        \n",
    "        # Fit the model to the factor analysis scores\n",
    "        cls[1].fit(scores)\n",
    "        \n",
    "        # List of assigned clusters\n",
    "        clusters = cls[1].fit_predict(scores)\n",
    "        \n",
    "        # Silhouette scores for each solution\n",
    "        silhouette_avg = silhouette_score(scores,clusters)\n",
    "        \n",
    "        # Store solution info\n",
    "        algorithm = cls[0]\n",
    "        n_stats = [algorithm, n, silhouette_avg, clusters]\n",
    "        sw.append(n_stats)\n",
    "\n",
    "    # Reorder cluster lists by descending silhouette scores.\n",
    "    # Clusters in first element should be assigned to training data.\n",
    "    sw = sorted(sw, key=itemgetter(2), reverse=True)\n",
    "    op[f'Optimal {sw[0][1]} cluster solution ({sw[0][0]})'] = sw[0][3] + 1\n",
    "    \n",
    "    \n",
    "    #**********************************************************************#\n",
    "    # This is where the classification stuff begins\n",
    "\n",
    "for i in range(18,last_var):\n",
    "\n",
    "\n",
    "    df_cl = op.iloc[:,np.r_[2:18,i]]  # i is the current cluster solution\n",
    "    df_cl_cons = op.iloc[:,np.r_[1:18,i]]  # Same as df_cl but with constant\n",
    "\n",
    "    #**********************************************************************#\n",
    "\n",
    "    # Split data into 70% training, 30% validation\n",
    "    train, valid = train_test_split(df_cl, test_size=0.30, random_state=123)\n",
    "\n",
    "    # X is unlabeled training data, y is true training labels \n",
    "    X, y = train.iloc[:,0:-1], train.iloc[:,-1]\n",
    "\n",
    "    X_valid, y_valid = valid.iloc[:,0:-1], valid.iloc[:,-1]\n",
    "\n",
    "    #**********************************************************************#\n",
    "\n",
    "    # Get variable importances\n",
    "\n",
    "    clf1 = RandomForestClassifier(random_state=0)\n",
    "    clf2 = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "    classifiers = [['rf', clf1], ['gbt', clf2]]\n",
    "\n",
    "    for classifier in classifiers:    \n",
    "        # Fit classifier to training data\n",
    "        classifier[1].fit(X,y)    \n",
    "\n",
    "    # Create variable importance dataframe\n",
    "    num_vars = list(range(1,len(clf1.feature_importances_)+1))\n",
    "    importance = pd.DataFrame({'variable': num_vars,\n",
    "                               'rf': clf1.feature_importances_,\n",
    "                               'gbt': clf2.feature_importances_,})\n",
    "\n",
    "    # Average variable importance of rf and gbt models\n",
    "    importance['avg'] = (importance['rf']+importance['gbt'])/2\n",
    "\n",
    "    # Put avg importances on a scale from 0 to 1 to make it easier to visualize\n",
    "    importance['Relative Importance'] = np.interp(importance['avg'],\n",
    "                                                  (importance['avg'].min(),\n",
    "                                                   importance['avg'].max()),\n",
    "                                                  (0, 1))\n",
    "\n",
    "    # View top 10 variables when RF and GBT models are averaged\n",
    "    top_10_avg = importance.sort_values(by='avg', ascending=False)[['avg','Relative Importance']].head(10)\n",
    "\n",
    "    # Add variable rank column to dataframe\n",
    "    importance_rank = num_vars\n",
    "    importance = importance.sort_values(by='Relative Importance', ascending=False)\n",
    "    importance['rank'] = importance_rank\n",
    "    importance.reset_index(inplace=True)\n",
    "\n",
    "    # Save index of top 5 variables (not the variable number!)\n",
    "    top_5 = importance[importance['rank'] <= 5]['index']\n",
    "\n",
    "    #**********************************************************************#\n",
    "    # Average and Standard Deviations for each cluster/variable combination\n",
    "    # For cluster 1 of 2, calculate the average and stdev for each variable\n",
    "    # For cluster 2 of 2, calculate the average and stdev for each variable\n",
    "    # Etc.\n",
    "\n",
    "    if n == max_clusters:\n",
    "\n",
    "        cls_avg_list = []\n",
    "\n",
    "        # Take the mean of every variable for each cluster\n",
    "        for k in range(1, df_cl_cons.iloc[:,-1].max()+1):\n",
    "            cls_mean = df_cl_cons[df_cl_cons.iloc[:,-1] == k].iloc[:,0:-1].mean()\n",
    "            cls_mean = cls_mean.append(pd.Series({\"Count\":df_cl[df_cl.iloc[:,-1] == k].iloc[:,0:-1].shape[0]}))\n",
    "            cls_avg_list.append(cls_mean)\n",
    "            cls_std = df_cl_cons[df_cl_cons.iloc[:,-1] == k].iloc[:,0:-1].std()\n",
    "            cls_avg_list.append(cls_std)\n",
    "            # NaN means there is either only 1 observation in that cluster or none.\n",
    "        \n",
    "        # Convert to dataframe and transpose\n",
    "        cls_averages = pd.DataFrame(cls_avg_list)\n",
    "        cls_averages = cls_averages.T\n",
    "\n",
    "        # Create helpful column names (Cluster # of total_#)\n",
    "        col_names = []\n",
    "        \n",
    "        for col in range(1, k+1):\n",
    "            new_name1 = f\"Avg cluster {col}/{k}\"\n",
    "            col_names.append(new_name1)\n",
    "            new_name2 = f\"Std cluster {col}/{k}\"\n",
    "            col_names.append(new_name2)            \n",
    "            \n",
    "        # Rename columns\n",
    "        cls_averages.columns = col_names\n",
    "\n",
    "        cls_averages_all = pd.concat([cls_averages_all, cls_averages], axis=1)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    #**********************************************************************#\n",
    "    # Convert data to binary, train classifiers, score validation, create maps\n",
    "\n",
    "    # Convert X, X_valid, and df_cl predictors to all 1 and -1\n",
    "    X = (X.mask(df > 0, other=1, inplace=False)\n",
    "         .mask(df <= 0, other=-1, inplace=False))\n",
    "    X_valid = (X_valid.mask(df > 0, other=1, inplace=False)\n",
    "               .mask(df <= 0, other=-1, inplace=False))\n",
    "    all_data_masked = (df_cl.iloc[:,0:-1].mask(df > 0, other=1, inplace=False)\n",
    "                       .mask(df <= 0, other=-1, inplace=False))\n",
    "\n",
    "    map_collection = []\n",
    "\n",
    "    # Retrain on the 2-5 most important variables\n",
    "    for j in range(2,6):\n",
    "\n",
    "        clf_scores = []\n",
    "\n",
    "        clf1 = RandomForestClassifier(random_state=0)\n",
    "        clf2 = GradientBoostingClassifier(random_state=0)\n",
    "        clf3 = SVC(random_state=0)\n",
    "        clf4 = KNeighborsClassifier()\n",
    "\n",
    "        classifiers = [['rf', clf1], ['gbt', clf2], ['svc', clf3], ['knn', clf4]]\n",
    "\n",
    "        # Fit each classifier to the current variable/cluster combination\n",
    "        for classifier in classifiers:\n",
    "\n",
    "            # Fit classifier to training data\n",
    "            classifier[1].fit(X.iloc[:,np.r_[top_5[0:j]]],y)\n",
    "\n",
    "            # Store classifier-specific results [algorithm object, classifier name, scores]\n",
    "            results = [classifier[1],\n",
    "                       classifier[0],\n",
    "                       classifier[1].score(X_valid.iloc[:,np.r_[top_5[0:j]]],y_valid)]\n",
    "\n",
    "            # Overall classifier results\n",
    "            clf_scores.append(results)\n",
    "\n",
    "        # Sort classifier accuracy in descending order\n",
    "        clf_scores = sorted(clf_scores, key=itemgetter(2), reverse=True)\n",
    "        # clf_scores[0][0] is the best model\n",
    "\n",
    "        # Fit the best model on all data\n",
    "        best_model = clf_scores[0][0].fit(all_data_masked.iloc[:,np.r_[top_5[0:j]]], df_cl.iloc[:,-1])\n",
    "\n",
    "        #******************************************************************#\n",
    "        # Create mappings\n",
    "\n",
    "        # Creates grid of dimension j\n",
    "        grid = pd.DataFrame(list(itertools.product([-1,1], repeat=j)))\n",
    "\n",
    "        grid.columns = all_data_masked.iloc[:,np.r_[top_5[0:j]]].columns\n",
    "\n",
    "        # This is the best model predicting the grid\n",
    "        preds = best_model.predict(grid)            \n",
    "\n",
    "        # Add to grid dataframe\n",
    "        grid['Predicted Cluster'] = preds\n",
    "\n",
    "        # Change grid to mapping to fit into the rest of the code\n",
    "        mapping = grid\n",
    "\n",
    "        # Save current mapping to map collection for this cluster solution\n",
    "        map_collection.append(mapping)\n",
    "\n",
    "        # Write each dataframe to a different worksheet.\n",
    "        mapping.to_excel(writer, index=False, sheet_name=f\"{df_cl.columns[-1][8:17]}s, {j} vars, {round(clf_scores[0][2]*100)}% Acc.\")\n",
    "\n",
    "    all_maps.append(map_collection)\n",
    "\n",
    "op.to_excel(writer, index=False, sheet_name=\"All Regressions, Clusters\")\n",
    "\n",
    "\n",
    "# Add averages for all observations to cls_averages_all before exporting\n",
    "all_obs = []\n",
    "\n",
    "# Variable means for all observations\n",
    "all_obs_mean = list(op.filter(regex='^[a-zA-Z][0-9]').mean().values)\n",
    "all_obs_mean.insert(0,op['Const'].mean())\n",
    "all_obs.append(all_obs_mean)\n",
    "\n",
    "cls_averages_all['new_col'] = pd.Series(list(op.filter(regex='^[a-zA-Z][0-9]').mean().values))\n",
    "\n",
    "# Variable standard deviations for all observations\n",
    "all_obs_std = list(op.filter(regex='^[a-zA-Z][0-9]').std().values)\n",
    "all_obs_std.insert(0,op['Const'].std())\n",
    "all_obs.append(all_obs_std)\n",
    "\n",
    "\n",
    "# Save as dataframe and append to all cls_averages_all dataframe\n",
    "all_obs_cols = list(op.filter(regex='^[a-zA-Z][0-9]').columns)\n",
    "all_obs_cols.insert(0, \"Const\")\n",
    "all_obs_df = pd.DataFrame(all_obs, columns=all_obs_cols)\n",
    "all_obs_df = all_obs_df.T\n",
    "all_obs_cols = ['All obs avg', 'All obs stdev']\n",
    "all_obs_df.columns = all_obs_cols\n",
    "cls_averages_all = pd.concat([cls_averages_all, all_obs_df], axis=1)\n",
    "\n",
    "\n",
    "cls_averages_all.to_excel(writer, sheet_name=\"Cluster Avgs and StDevs\")\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of script. Code below is for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for making the letter-based regression dynamic\n",
    "\n",
    "# Create a list of unique first letters of variables\n",
    "X = df.drop(['UID','Rating','target'], axis=1)\n",
    "y = df[\"target\"]\n",
    "\n",
    "var_letters = []\n",
    "\n",
    "for i in X.columns:\n",
    "    var_letters.append(i[0:1])  # Append first character\n",
    "\n",
    "var_letters = list(np.unique(var_letters))  # List of unique variable letters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Group Regressions & PCA Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "### Hidden: Code for the regressions on variable groups A, B, C, D\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# ### Linear Regression with A Variables\n",
    "\n",
    "# # Unique IDs\n",
    "# ids = df.UID.unique()\n",
    "\n",
    "# # Run linear regressions for each UID\n",
    "# op = pd.DataFrame\n",
    "# intercept = []\n",
    "# coefficients=[]\n",
    "# UID = []\n",
    "# for i in ids:\n",
    "#     df_i = df[df.UID == i]              # Create dataframe for current user id\n",
    "#     X = df_i.drop(['UID','B1', 'B2', 'B3', 'B4','C1', 'C2', 'C3', 'C4','D1', 'D2', 'D3', 'D4','Rating','target'], axis=1)  # df input variables only\n",
    "#     y = df_i['target']                  # Series of target variable\n",
    "#     reg = LinearRegression().fit(X, y)  # Fit linear regression\n",
    "#     reg.score(X, y)                     # Score regression model\n",
    "#     unique_id=df_i['UID'].unique()      # Saves current user id\n",
    "#     const = reg.intercept_              # Save intercept of the regression model\n",
    "#     coef = reg.coef_                    # Coefficients of regression model\n",
    "#     UID.append(unique_id)               # Append current user id\n",
    "#     intercept.append(const)             # Append current intercept\n",
    "#     coefficients.append(coef)           # Append current regression coefficients\n",
    "\n",
    "# # Convert newly created lists into dataframes\n",
    "# intercep_new = pd.DataFrame(intercept)\n",
    "# coefficients_new = pd.DataFrame(coefficients)\n",
    "# UID_new = pd.DataFrame(UID)\n",
    "\n",
    "# # Get columns names\n",
    "# colNames = df.drop(['B1', 'B2', 'B3', 'B4','C1', 'C2', 'C3', 'C4','D1', 'D2', 'D3', 'D4','Rating', 'target'], axis=1).columns\n",
    "# colNames = colNames.insert(1, 'Const')\n",
    "# colNames\n",
    "\n",
    "# # Concatenate the new dataframes and add column names\n",
    "# op = pd.concat([UID_new,intercep_new, coefficients_new], axis=1)\n",
    "# op.columns = colNames\n",
    "# op_A = op.iloc[:,2:6]\n",
    "\n",
    "\n",
    "# #******************************************************************************#\n",
    "# ### Linear Regression with B Variables\n",
    "\n",
    "# # Unique IDs\n",
    "# ids = df.UID.unique()\n",
    "\n",
    "# # Run linear regressions for each UID\n",
    "# op = pd.DataFrame\n",
    "# intercept = []\n",
    "# coefficients=[]\n",
    "# UID = []\n",
    "# for i in ids:\n",
    "#     df_i = df[df.UID == i]              # Create dataframe for current user id\n",
    "#     X = df_i.drop(['UID','A1', 'A2', 'A3', 'A4','C1', 'C2', 'C3', 'C4','D1', 'D2', 'D3', 'D4','Rating','target'], axis=1)  # df input variables only\n",
    "#     y = df_i['target']                  # Series of target variable\n",
    "#     reg = LinearRegression().fit(X, y)  # Fit linear regression\n",
    "#     reg.score(X, y)                     # Score regression model\n",
    "#     unique_id=df_i['UID'].unique()      # Saves current user id\n",
    "#     const = reg.intercept_              # Save intercept of the regression model\n",
    "#     coef = reg.coef_                    # Coefficients of regression model\n",
    "#     UID.append(unique_id)               # Append current user id\n",
    "#     intercept.append(const)             # Append current intercept\n",
    "#     coefficients.append(coef)           # Append current regression coefficients\n",
    "\n",
    "# # Convert newly created lists into dataframes\n",
    "# intercep_new = pd.DataFrame(intercept)\n",
    "# coefficients_new = pd.DataFrame(coefficients)\n",
    "# UID_new = pd.DataFrame(UID)\n",
    "\n",
    "# # Get columns names\n",
    "# colNames = df.drop(['A1', 'A2', 'A3', 'A4','C1', 'C2', 'C3', 'C4','D1', 'D2', 'D3', 'D4','Rating', 'target'], axis=1).columns\n",
    "# colNames = colNames.insert(1, 'Const')\n",
    "# colNames\n",
    "\n",
    "# # Concatenate the new dataframes and add column names\n",
    "# op = pd.concat([UID_new,intercep_new, coefficients_new], axis=1)\n",
    "# op.columns = colNames\n",
    "# op_B = op.iloc[:,2:6]\n",
    "\n",
    "\n",
    "# #******************************************************************************#\n",
    "# ### Linear Regression with C Variables\n",
    "\n",
    "# # Unique IDs\n",
    "# ids = df.UID.unique()\n",
    "\n",
    "# # Run linear regressions for each UID\n",
    "# op = pd.DataFrame\n",
    "# intercept = []\n",
    "# coefficients=[]\n",
    "# UID = []\n",
    "# for i in ids:\n",
    "#     df_i = df[df.UID == i]              # Create dataframe for current user id\n",
    "#     X = df_i.drop(['UID', 'A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'B3', 'B4', 'D1', 'D2', 'D3', 'D4', 'Rating', 'target'], axis=1)  # df input variables only\n",
    "#     y = df_i['target']                  # Series of target variable\n",
    "#     reg = LinearRegression().fit(X, y)  # Fit linear regression\n",
    "#     reg.score(X, y)                     # Score regression model\n",
    "#     unique_id=df_i['UID'].unique()      # Saves current user id\n",
    "#     const = reg.intercept_              # Save intercept of the regression model\n",
    "#     coef = reg.coef_                    # Coefficients of regression model\n",
    "#     UID.append(unique_id)               # Append current user id\n",
    "#     intercept.append(const)             # Append current intercept\n",
    "#     coefficients.append(coef)           # Append current regression coefficients\n",
    "\n",
    "# # Convert newly created lists into dataframes\n",
    "# intercep_new = pd.DataFrame(intercept)\n",
    "# coefficients_new = pd.DataFrame(coefficients)\n",
    "# UID_new = pd.DataFrame(UID)\n",
    "\n",
    "# # Get columns names\n",
    "# colNames = df.drop(['A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'B3', 'B4', 'D1', 'D2', 'D3', 'D4', 'Rating', 'target'], axis=1).columns\n",
    "# colNames = colNames.insert(1, 'Const')\n",
    "# colNames\n",
    "\n",
    "# # Concatenate the new dataframes and add column names\n",
    "# op = pd.concat([UID_new,intercep_new, coefficients_new], axis=1)\n",
    "# op.columns = colNames\n",
    "# op_C = op.iloc[:,2:6]\n",
    "\n",
    "\n",
    "\n",
    "# #******************************************************************************#\n",
    "# ### Linear Regression with D Variables\n",
    "\n",
    "# # Unique IDs\n",
    "# ids = df.UID.unique()\n",
    "\n",
    "# # Run linear regressions for each UID\n",
    "# op = pd.DataFrame\n",
    "# intercept = []\n",
    "# coefficients=[]\n",
    "# UID = []\n",
    "# for i in ids:\n",
    "#     df_i = df[df.UID == i]              # Create dataframe for current user id\n",
    "#     X = df_i.drop(['UID', 'A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'B3', 'B4', 'C1', 'C2', 'C3', 'C4', 'Rating', 'target'], axis=1)  # df input variables only\n",
    "#     y = df_i['target']                  # Series of target variable\n",
    "#     reg = LinearRegression().fit(X, y)  # Fit linear regression\n",
    "#     reg.score(X, y)                     # Score regression model\n",
    "#     unique_id=df_i['UID'].unique()      # Saves current user id\n",
    "#     const = reg.intercept_              # Save intercept of the regression model\n",
    "#     coef = reg.coef_                    # Coefficients of regression model\n",
    "#     UID.append(unique_id)               # Append current user id\n",
    "#     intercept.append(const)             # Append current intercept\n",
    "#     coefficients.append(coef)           # Append current regression coefficients\n",
    "\n",
    "# # Convert newly created lists into dataframes\n",
    "# intercep_new = pd.DataFrame(intercept)\n",
    "# coefficients_new = pd.DataFrame(coefficients)\n",
    "# UID_new = pd.DataFrame(UID)\n",
    "\n",
    "# # Get columns names\n",
    "# colNames = df.drop(['A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'B3', 'B4', 'C1', 'C2', 'C3', 'C4', 'Rating', 'target'], axis=1).columns\n",
    "# colNames = colNames.insert(1, 'Const')\n",
    "# colNames\n",
    "\n",
    "# # Concatenate the new dataframes and add column names\n",
    "# op = pd.concat([UID_new,intercep_new, coefficients_new], axis=1)\n",
    "# op.columns = colNames\n",
    "# op_D = op.iloc[:,2:6]\n",
    "\n",
    "\n",
    "# #******************************************************************************#\n",
    "# ### Replace 1's w/ Regression Coefficients in Original Data\n",
    "\n",
    "# # Concatenate regression dataframes\n",
    "# all_cfs = pd.concat([op_A, op_B, op_C, op_D], axis=1)\n",
    "\n",
    "# # Replace 1's w/ regression coefficients by column\n",
    "# cfs_cols = all_cfs.columns\n",
    "\n",
    "# for col in cfs_cols:\n",
    "#     for i in range(1,len(all_cfs)+1):\n",
    "#         df.loc[df['UID'] == i,[col]] = df.loc[df['UID'] == i,[col]].replace(1,all_cfs.loc[i-1,col])\n",
    "\n",
    "# df.head()\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "## PCA on Regression Coefficients\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# #******************************************************************************#\n",
    "# # Comparing covariance and correlation PCA.  We should do correlation PCA.\n",
    "\n",
    "# # Create PCA dataframe\n",
    "# df_fct = op.drop(['UID','Const'], axis=1)\n",
    "\n",
    "# # Standardize df_fct\n",
    "# # df_pca = stats.zscore(df_fct)  # Didn't need to worry about standardizing.\n",
    "# # Could build in a correlation/covariance PCA thing using ranges of the variables\n",
    "\n",
    "# # Create PCA object\n",
    "# pca = PCA(random_state=123)\n",
    "\n",
    "# # Get principal components\n",
    "# pca.fit(df_fct)\n",
    "\n",
    "# # Get scores\n",
    "# pca.transform(df_fct)\n",
    "\n",
    "# # Components needed\n",
    "# pcs_needed = len(np.where(pca.explained_variance_ >= 1)[0])\n",
    "\n",
    "# # Save scores for PCs w/ eignvalues >=1 as dataframe for clustering\n",
    "# scores = pd.DataFrame(pca.transform(df_fct))\n",
    "# scores = scores.iloc[:,0:pcs_needed]\n",
    "\n",
    "# # How many observations\n",
    "# n_samples = pca.components_.shape[0]\n",
    "\n",
    "# # Transpose the principal components\n",
    "# murph = pca.components_.T\n",
    "# # Center the data\n",
    "# murph -= np.mean(murph, axis=0)\n",
    "# # Compute the covariance matrix\n",
    "# cov_matrix = np.dot(murph.T, murph) / n_samples\n",
    "\n",
    "# for eigenvector in pca.components_:\n",
    "#     print(np.dot(eigenvector.T, np.dot(cov_matrix, eigenvector)))\n",
    "\n",
    "# n_samples = df_fct.shape[0]\n",
    "\n",
    "# pca = PCA(random_state=123)\n",
    "# X_transformed = pca.fit_transform(df_fct)\n",
    "\n",
    "# # This is the explained variance.  They are big numbers because this is Cov. PCA\n",
    "# X_centered = df_fct - np.mean(df_fct, axis=0)\n",
    "# cov_matrix = np.dot(X_centered.T, X_centered) / n_samples\n",
    "# eigenvalues = pca.explained_variance_\n",
    "# for eigenvalue, eigenvector in zip(eigenvalues, pca.components_):\n",
    "#     print(eigenvalue)\n",
    "\n",
    "# # This is the same thing as the code above but built into sklearn\n",
    "# pca.explained_variance_\n",
    "# # These are the eigenvalues of the covariance matrix\n",
    "# # They don't have values near 1-ish, so I need to understand why.\n",
    "\n",
    "\n",
    "# #******************************************************************************#\n",
    "# # Biplots for comparing correlation and covariance PCA for this data\n",
    "\n",
    "# # !pip install pca\n",
    "# from pca import pca\n",
    "\n",
    "# # Correlation PCA & biplot\n",
    "# model = pca(n_components=4)\n",
    "# results = model.fit_transform(df_pca)\n",
    "# fig, ax = model.biplot(n_feat=16, cmap=None, label=False, legend=False)\n",
    "\n",
    "# # Covariance PCA & biplot\n",
    "# model2 = pca(n_components=4)\n",
    "# results2 = model2.fit_transform(df_fct)\n",
    "# fig, ax = model2.biplot(n_feat=16, cmap=None, label=False, legend=False)\n",
    "\n",
    "# # Create PCA dataframe\n",
    "# df_fct = op.drop(['UID','Const'], axis=1)\n",
    "\n",
    "# # Standardize df_fct - Need to standardize to use eigenvalues >= 1\n",
    "# df_fct = stats.zscore(df_fct)\n",
    "\n",
    "# # Create PCA object\n",
    "# pca = PCA(random_state=123)\n",
    "\n",
    "# # Get principal components & scores\n",
    "# pca.fit_transform(df_fct)\n",
    "\n",
    "# # Components needed\n",
    "# pcs_needed = len(np.where(pca.explained_variance_ >= 1)[0])\n",
    "\n",
    "# # Save scores for PCs w/ eignvalues >=1 as dataframe for clustering\n",
    "# scores = pd.DataFrame(pca.transform(df_fct))\n",
    "# scores = scores.iloc[:,0:pcs_needed]\n",
    "# expl_var = round(pca.explained_variance_ratio_.cumsum()[pcs_needed-1]*100, 2)\n",
    "\n",
    "# print(\"Principal Components Used: \", pcs_needed, sep='')\n",
    "# print(\"Variance Explained: \", expl_var, \"%\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with SMOTE for synthetic data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# EXPERIMENTING WITH SMOTE\n",
    "################################################################################\n",
    "\n",
    "# Maximum number of clusters\n",
    "max_clusters = 6\n",
    "\n",
    "for n in range(2, max_clusters+1):\n",
    "    \n",
    "    sw = []\n",
    "    \n",
    "    # Create clustering objects\n",
    "    cls1 = KMeans(n_clusters=n, random_state=0)\n",
    "    cls2 = KMedoids(n_clusters=n, random_state=0)\n",
    "    cls3 = AgglomerativeClustering(n_clusters=n,\n",
    "                                   affinity='euclidean',\n",
    "                                   linkage='ward')\n",
    "        # Agglomerative clustering: if linkage=ward, affinity must be Euclidean\n",
    "    cls_algs = [['kMeans', cls1],\n",
    "                ['kMedoids', cls2],\n",
    "                ['Hierarchical', cls3]]\n",
    "    \n",
    "    # Fit and score clustering solutions for i clusters w/ each algorithm\n",
    "    for cls in cls_algs:\n",
    "        \n",
    "        # Fit the model to the factor analysis scores\n",
    "        cls[1].fit(scores)\n",
    "        \n",
    "        # List of assigned clusters\n",
    "        clusters = cls[1].fit_predict(scores)\n",
    "        \n",
    "        # Silhouette scores for each solution\n",
    "        silhouette_avg = silhouette_score(scores,clusters)\n",
    "        \n",
    "        # Store solution info\n",
    "        algorithm = cls[0]\n",
    "        n_stats = [algorithm, n, silhouette_avg, clusters]\n",
    "        sw.append(n_stats)\n",
    "\n",
    "    # Reorder cluster lists by descending silhouette scores.\n",
    "    # Clusters in first element should be assigned to training data.\n",
    "    sw = sorted(sw, key=itemgetter(2), reverse=True)\n",
    "    op[f'Optimal {sw[0][1]} cluster solution ({sw[0][0]})'] = sw[0][3] + 1\n",
    "\n",
    "import imblearn\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = op.filter(regex='^[a-zA-Z][0-9]')\n",
    "y = op.iloc[:,18]\n",
    "\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "op.head()\n",
    "\n",
    "X['Optimal 2 cluster solution (Hierarchical)'] = y\n",
    "X['UID'] = op.UID\n",
    "X['Const'] = op.Const\n",
    "# X = X.iloc[:, np.r_[-2,-1,0:-2]]\n",
    "\n",
    "cols = X.columns.tolist()\n",
    "cols = cols[-2:] + cols[:-2]\n",
    "\n",
    "X = X[cols]\n",
    "\n",
    "op = X\n",
    "\n",
    "op.shape\n",
    "\n",
    "op.head()\n",
    "\n",
    "for i in range(18,19):\n",
    "\n",
    "\n",
    "    df_cl = op.iloc[:,np.r_[2:18,i]]  # i is the current cluster solution\n",
    "\n",
    "    #**********************************************************************#\n",
    "\n",
    "    # Split data into 70% training, 30% validation\n",
    "    train, valid = train_test_split(df_cl, test_size=0.30, random_state=123)\n",
    "\n",
    "    # X is unlabeled training data, y is true training labels \n",
    "    X, y = train.iloc[:,0:-1], train.iloc[:,-1]\n",
    "\n",
    "    X_valid, y_valid = valid.iloc[:,0:-1], valid.iloc[:,-1]\n",
    "\n",
    "    #**********************************************************************#\n",
    "\n",
    "    # Get variable importances\n",
    "\n",
    "    clf1 = RandomForestClassifier(random_state=0)\n",
    "    clf2 = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "    classifiers = [['rf', clf1], ['gbt', clf2]]\n",
    "\n",
    "    for classifier in classifiers:    \n",
    "        # Fit classifier to training data\n",
    "        classifier[1].fit(X,y)    \n",
    "\n",
    "    # Create variable importance dataframe\n",
    "    num_vars = list(range(1,len(clf1.feature_importances_)+1))\n",
    "    importance = pd.DataFrame({'variable': num_vars,\n",
    "                               'rf': clf1.feature_importances_,\n",
    "                               'gbt': clf2.feature_importances_,})\n",
    "\n",
    "    # Average variable importance of rf and gbt models\n",
    "    importance['avg'] = (importance['rf']+importance['gbt'])/2\n",
    "\n",
    "    # Put avg importances on a scale from 0 to 1 to make it easier to visualize\n",
    "    importance['Relative Importance'] = np.interp(importance['avg'],\n",
    "                                                  (importance['avg'].min(),\n",
    "                                                   importance['avg'].max()),\n",
    "                                                  (0, 1))\n",
    "\n",
    "    # View top 10 variables when RF and GBT models are averaged\n",
    "    top_10_avg = importance.sort_values(by='avg', ascending=False)[['avg','Relative Importance']].head(10)\n",
    "\n",
    "    # Add variable rank column to dataframe\n",
    "    importance_rank = num_vars\n",
    "    importance = importance.sort_values(by='Relative Importance', ascending=False)\n",
    "    importance['rank'] = importance_rank\n",
    "    importance.reset_index(inplace=True)\n",
    "\n",
    "    # Save index of top 5 variables (not the variable number!)\n",
    "    top_5 = importance[importance['rank'] <= 5]['index']\n",
    "\n",
    "    #**********************************************************************#\n",
    "    # Average and Standard Deviations for each cluster/variable combination\n",
    "    # For cluster 1 of 2, calculate the average and stdev for each variable\n",
    "    # For cluster 2 of 2, calculate the average and stdev for each variable\n",
    "    # Etc.\n",
    "\n",
    "#     if n == max_clusters:\n",
    "\n",
    "#         cls_avg_list = []\n",
    "\n",
    "#         # Take the mean of every variable for each cluster\n",
    "#         for k in range(1, df_cl_cons.iloc[:,-1].max()+1):\n",
    "#             cls_mean = df_cl_cons[df_cl_cons.iloc[:,-1] == k].iloc[:,0:-1].mean()\n",
    "#             cls_mean = cls_mean.append(pd.Series({\"Count\":df_cl[df_cl.iloc[:,-1] == k].iloc[:,0:-1].shape[0]}))\n",
    "#             cls_avg_list.append(cls_mean)\n",
    "#             cls_std = df_cl_cons[df_cl_cons.iloc[:,-1] == k].iloc[:,0:-1].std()\n",
    "#             cls_avg_list.append(cls_std)\n",
    "#             # NaN means there is either only 1 observation in that cluster or none.\n",
    "        \n",
    "#         # Convert to dataframe and transpose\n",
    "#         cls_averages = pd.DataFrame(cls_avg_list)\n",
    "#         cls_averages = cls_averages.T\n",
    "\n",
    "#         # Create helpful column names (Cluster # of total_#)\n",
    "#         col_names = []\n",
    "        \n",
    "#         for col in range(1, k+1):\n",
    "#             new_name1 = f\"Avg cluster {col}/{k}\"\n",
    "#             col_names.append(new_name1)\n",
    "#             new_name2 = f\"Std cluster {col}/{k}\"\n",
    "#             col_names.append(new_name2)            \n",
    "            \n",
    "#         # Rename columns\n",
    "#         cls_averages.columns = col_names\n",
    "\n",
    "#         cls_averages_all = pd.concat([cls_averages_all, cls_averages], axis=1)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    #**********************************************************************#\n",
    "    # Convert data to binary, train classifiers, score validation, create maps\n",
    "\n",
    "    # Convert X, X_valid, and df_cl predictors to all 1 and -1\n",
    "#     X = (X.mask(df > 0, other=1, inplace=False)\n",
    "#          .mask(df <= 0, other=-1, inplace=False))\n",
    "#     X_valid = (X_valid.mask(df > 0, other=1, inplace=False)\n",
    "#                .mask(df <= 0, other=-1, inplace=False))\n",
    "    all_data_masked = (df_cl.iloc[:,0:-1]) #.mask(df > 0, other=1, inplace=False)\n",
    "#                        .mask(df <= 0, other=-1, inplace=False))\n",
    "\n",
    "    map_collection = []\n",
    "\n",
    "    # Retrain on the 2-5 most important variables\n",
    "    for j in range(2,6):\n",
    "\n",
    "        clf_scores = []\n",
    "\n",
    "        clf1 = RandomForestClassifier(random_state=0)\n",
    "        clf2 = GradientBoostingClassifier(random_state=0)\n",
    "        clf3 = SVC(random_state=0)\n",
    "        clf4 = KNeighborsClassifier()\n",
    "\n",
    "        classifiers = [['rf', clf1], ['gbt', clf2], ['svc', clf3], ['knn', clf4]]\n",
    "\n",
    "        # Fit each classifier to the current variable/cluster combination\n",
    "        for classifier in classifiers:\n",
    "\n",
    "            # Fit classifier to training data\n",
    "            classifier[1].fit(X.iloc[:,np.r_[top_5[0:j]]],y)\n",
    "            print(classifier[1].predict(X.iloc[:,np.r_[top_5[0:j]]]))\n",
    "\n",
    "            # Store classifier-specific results [algorithm object, classifier name, scores]\n",
    "            results = [classifier[1],\n",
    "                       classifier[0],\n",
    "                       classifier[1].score(X_valid.iloc[:,np.r_[top_5[0:j]]],y_valid)]\n",
    "\n",
    "            # Overall classifier results\n",
    "            clf_scores.append(results)\n",
    "\n",
    "        # Sort classifier accuracy in descending order\n",
    "        clf_scores = sorted(clf_scores, key=itemgetter(2), reverse=True)\n",
    "        # clf_scores[0][0] is the best model\n",
    "\n",
    "        # Fit the best model on all data\n",
    "        best_model = clf_scores[0][0].fit(all_data_masked.iloc[:,np.r_[top_5[0:j]]], df_cl.iloc[:,-1])\n",
    "        \n",
    "        print(j)\n",
    "        print(clf_scores)\n",
    "\n",
    "#         #******************************************************************#\n",
    "#         # Create mappings\n",
    "\n",
    "#         # Creates grid of dimension j\n",
    "#         grid = pd.DataFrame(list(itertools.product([-1,1], repeat=j)))\n",
    "\n",
    "#         grid.columns = all_data_masked.iloc[:,np.r_[top_5[0:j]]].columns\n",
    "\n",
    "#         # This is the best model predicting the grid\n",
    "#         preds = best_model.predict(grid)            \n",
    "\n",
    "#         # Add to grid dataframe\n",
    "#         grid['Predicted Cluster'] = preds\n",
    "\n",
    "#         # Change grid to mapping to fit into the rest of the code\n",
    "#         mapping = grid\n",
    "\n",
    "#         # Save current mapping to map collection for this cluster solution\n",
    "#         map_collection.append(mapping)\n",
    "\n",
    "#         # Write each dataframe to a different worksheet.\n",
    "#         mapping.to_excel(writer, index=False, sheet_name=f\"{df_cl.columns[-1][8:17]}s, {j} vars, {round(clf_scores[0][2]*100)}% Acc.\")\n",
    "\n",
    "#     all_maps.append(map_collection)\n",
    "\n",
    "# op.to_excel(writer, index=False, sheet_name=\"All Regressions, Clusters\")\n",
    "\n",
    "\n",
    "# # Add averages for all observations to cls_averages_all before exporting\n",
    "# all_obs = []\n",
    "\n",
    "# # Variable means for all observations\n",
    "# all_obs_mean = list(op.filter(regex='^[a-zA-Z][0-9]').mean().values)\n",
    "# all_obs_mean.insert(0,op['Const'].mean())\n",
    "# all_obs.append(all_obs_mean)\n",
    "\n",
    "# cls_averages_all['new_col'] = pd.Series(list(op.filter(regex='^[a-zA-Z][0-9]').mean()).values\n",
    "\n",
    "# # Variable standard deviations for all observations\n",
    "# all_obs_std = list(op.filter(regex='^[a-zA-Z][0-9]').std().values)\n",
    "# all_obs_std.insert(0,op['Const'].std())\n",
    "# all_obs.append(all_obs_std)\n",
    "\n",
    "\n",
    "# # Save as dataframe and append to all cls_averages_all dataframe\n",
    "# all_obs_cols = list(op.filter(regex='^[a-zA-Z][0-9]').columns)\n",
    "# all_obs_cols.insert(0, \"Const\")\n",
    "# all_obs_df = pd.DataFrame(all_obs, columns=all_obs_cols)\n",
    "# all_obs_df = all_obs_df.T\n",
    "# all_obs_cols = ['All obs avg', 'All obs stdev']\n",
    "# all_obs_df.columns = all_obs_cols\n",
    "# cls_averages_all = pd.concat([cls_averages_all, all_obs_df], axis=1)\n",
    "\n",
    "\n",
    "# cls_averages_all.to_excel(writer, sheet_name=\"Cluster Avgs and StDevs\")\n",
    "\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Scenario Analysis</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>Rating</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID  A1  A2  A3  A4  B1  B2  B3  B4  C1  C2  C3  C4  D1  D2  D3  D4  \\\n",
       "0    1   1   0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   \n",
       "1    1   1   0   0   0   0   0   1   0   0   0   0   0   1   0   0   0   \n",
       "2    1   0   0   0   1   0   0   0   1   1   0   0   0   0   0   0   0   \n",
       "3    1   0   1   0   0   0   0   0   0   0   0   0   1   0   1   0   0   \n",
       "4    1   0   1   0   0   0   0   1   0   0   1   0   0   0   0   0   1   \n",
       "\n",
       "   Rating      target  \n",
       "0     100  100.000507  \n",
       "1     100  100.000668  \n",
       "2       0    0.000386  \n",
       "3       0    0.000067  \n",
       "4       0    0.000382  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where(condition, value if condition is true, value if condition is false)\n",
    "\n",
    "# create a list of the column categories\n",
    "\n",
    "cat_A = ['A1', 'A2', 'A3', 'A4']\n",
    "cat_B = ['B1', 'B2', 'B3', 'B4']\n",
    "cat_C = ['C1', 'C2', 'C3', 'C4']\n",
    "cat_D = ['D1', 'D2', 'D3', 'D4']\n",
    "\n",
    "# create a list of our conditions\n",
    "cat_A_conditions = [\n",
    "    (df['A1'] == 1),\n",
    "    (df['A2'] == 1),\n",
    "    (df['A3'] == 1),\n",
    "    (df['A4'] == 1),\n",
    "    (df['A1']==0) & (df['A2']==0) & (df['A3']==0) & (df['A4']==0),\n",
    "    ]\n",
    "\n",
    "cat_B_conditions = [    \n",
    "    (df['B1'] == 1),\n",
    "    (df['B2'] == 1),\n",
    "    (df['B3'] == 1),\n",
    "    (df['B4'] == 1),\n",
    "    (df['B1']==0) & (df['B2']==0) & (df['B3']==0) & (df['B4']==0),\n",
    "    ]    \n",
    "\n",
    "cat_C_conditions = [    \n",
    "    (df['C1'] == 1),\n",
    "    (df['C2'] == 1),\n",
    "    (df['C3'] == 1),\n",
    "    (df['C4'] == 1),\n",
    "    (df['C1']==0) & (df['C2']==0) & (df['C3']==0) & (df['C4']==0),\n",
    "    ]\n",
    "\n",
    "cat_D_conditions = [    \n",
    "    (df['D1'] == 1),\n",
    "    (df['D2'] == 1),\n",
    "    (df['D3'] == 1),\n",
    "    (df['D4'] == 1),  \n",
    "    (df['D1']==0) & (df['D2']==0) & (df['D3']==0) & (df['D4']==0),\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "cat_A_values = ['A1', 'A2', 'A3', 'A4', 'A0']\n",
    "cat_B_values = ['B1', 'B2', 'B3', 'B4','B0']\n",
    "cat_C_values = ['C1', 'C2', 'C3', 'C4','C0']\n",
    "cat_D_values = ['D1', 'D2', 'D3', 'D4','D0']\n",
    "    \n",
    "df['cat_A_scenario'] = np.select(cat_A_conditions, cat_A_values)\n",
    "df['cat_B_scenario'] = np.select(cat_B_conditions, cat_B_values)\n",
    "df['cat_C_scenario'] = np.select(cat_C_conditions, cat_C_values)\n",
    "df['cat_D_scenario'] = np.select(cat_D_conditions, cat_D_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Example of Scenario where Variable A1 has a 1 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_A = df['cat_A_scenario'].unique()\n",
    "# list of independant variables for regression\n",
    "fields = df.columns[1:17]\n",
    "dep_var = df['noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>C1</th>\n",
       "      <th>...</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>Rating</th>\n",
       "      <th>noise</th>\n",
       "      <th>cat_A_scenario</th>\n",
       "      <th>cat_B_scenario</th>\n",
       "      <th>cat_C_scenario</th>\n",
       "      <th>cat_D_scenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000893</td>\n",
       "      <td>A1</td>\n",
       "      <td>B4</td>\n",
       "      <td>C1</td>\n",
       "      <td>D0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000783</td>\n",
       "      <td>A1</td>\n",
       "      <td>B3</td>\n",
       "      <td>C0</td>\n",
       "      <td>D1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000825</td>\n",
       "      <td>A1</td>\n",
       "      <td>B0</td>\n",
       "      <td>C1</td>\n",
       "      <td>D4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>A1</td>\n",
       "      <td>B4</td>\n",
       "      <td>C4</td>\n",
       "      <td>D3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000252</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>C3</td>\n",
       "      <td>D4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000552</td>\n",
       "      <td>A1</td>\n",
       "      <td>B4</td>\n",
       "      <td>C0</td>\n",
       "      <td>D4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000693</td>\n",
       "      <td>A1</td>\n",
       "      <td>B3</td>\n",
       "      <td>C2</td>\n",
       "      <td>D3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000722</td>\n",
       "      <td>A1</td>\n",
       "      <td>B4</td>\n",
       "      <td>C4</td>\n",
       "      <td>D2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000264</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>C4</td>\n",
       "      <td>D0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000156</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>C4</td>\n",
       "      <td>D0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>513 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      UID  A1  A2  A3  A4  B1  B2  B3  B4  C1  ...  D1  D2  D3  D4  Rating  \\\n",
       "0       1   1   0   0   0   0   0   0   1   1  ...   0   0   0   0     100   \n",
       "1       1   1   0   0   0   0   0   1   0   0  ...   1   0   0   0     100   \n",
       "13      1   1   0   0   0   0   0   0   0   1  ...   0   0   0   1     100   \n",
       "16      1   1   0   0   0   0   0   0   1   0  ...   0   0   1   0       0   \n",
       "17      1   1   0   0   0   1   0   0   0   0  ...   0   0   0   1     100   \n",
       "...   ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..     ...   \n",
       "2383  100   1   0   0   0   0   0   0   1   0  ...   0   0   0   1       5   \n",
       "2385  100   1   0   0   0   0   0   1   0   0  ...   0   0   1   0       5   \n",
       "2393  100   1   0   0   0   0   0   0   1   0  ...   0   1   0   0       5   \n",
       "2398  100   1   0   0   0   1   0   0   0   0  ...   0   0   0   0       5   \n",
       "2399  100   1   0   0   0   1   0   0   0   0  ...   0   0   0   0       5   \n",
       "\n",
       "           noise  cat_A_scenario  cat_B_scenario  cat_C_scenario  \\\n",
       "0     100.000893              A1              B4              C1   \n",
       "1     100.000783              A1              B3              C0   \n",
       "13    100.000825              A1              B0              C1   \n",
       "16      0.000479              A1              B4              C4   \n",
       "17    100.000252              A1              B1              C3   \n",
       "...          ...             ...             ...             ...   \n",
       "2383    5.000552              A1              B4              C0   \n",
       "2385    5.000693              A1              B3              C2   \n",
       "2393    5.000722              A1              B4              C4   \n",
       "2398    5.000264              A1              B1              C4   \n",
       "2399    5.000156              A1              B1              C4   \n",
       "\n",
       "     cat_D_scenario  \n",
       "0                D0  \n",
       "1                D1  \n",
       "13               D4  \n",
       "16               D3  \n",
       "17               D4  \n",
       "...             ...  \n",
       "2383             D4  \n",
       "2385             D3  \n",
       "2393             D2  \n",
       "2398             D0  \n",
       "2399             D0  \n",
       "\n",
       "[513 rows x 23 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_A1 = df[df['cat_A_scenario']=='A1']\n",
    "df_A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.91035518762444\n",
      "[ -3.84235504 -13.45891789  -5.98115784  -1.00151876  -5.2084735\n",
      "  -0.95495595  -3.15645822  -7.2888877   -2.32297686  -3.7339574\n",
      "  -5.08132817   1.23153583]\n"
     ]
    }
   ],
   "source": [
    "X =df_A1[['B1', 'B2', 'B3', 'B4','C1', 'C2', 'C3', 'C4','D1', 'D2', 'D3', 'D4']]\n",
    "y= df_A1['noise']\n",
    "reg = LinearRegression().fit(X, y)\n",
    "reg.score(X, y)\n",
    "const = reg.intercept_\n",
    "coef = reg.coef_\n",
    "print(const)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch pad, not finished\n",
    "for i in cat_A:\n",
    "    df_i = df[df.cat_A_scenario == i]\n",
    "    X =df_i[df_i.columns[1:17]]\n",
    "    y= df_i['noise']\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    reg.score(X, y)\n",
    "    const = reg.intercept_\n",
    "    coef = reg.coef_\n",
    "    intercept.append(const)\n",
    "    coefficients.append(coef)\n",
    "#print(UID)    \n",
    "intercep_new = pd.DataFrame(intercept)\n",
    "coefficients_new = pd.DataFrame(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
