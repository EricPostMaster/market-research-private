{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install streamlit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xlsxwriter\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "# conda install -c conda-forge scikit-learn-extra\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from operator import itemgetter\n",
    "\n",
    "# Principal Components Analysis\n",
    "from scipy import stats\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# Classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyclustering\n",
      "  Downloading pyclustering-0.10.1.2.tar.gz (2.6 MB)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\ericd\\anaconda3\\lib\\site-packages (from pyclustering) (1.4.1)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\ericd\\appdata\\roaming\\python\\python37\\site-packages (from pyclustering) (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.15.2 in c:\\users\\ericd\\appdata\\roaming\\python\\python37\\site-packages (from pyclustering) (1.20.1)\n",
      "Requirement already satisfied: Pillow>=5.2.0 in c:\\users\\ericd\\appdata\\roaming\\python\\python37\\site-packages (from pyclustering) (8.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\ericd\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib>=3.0.0->pyclustering) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\ericd\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib>=3.0.0->pyclustering) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ericd\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib>=3.0.0->pyclustering) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ericd\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib>=3.0.0->pyclustering) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\ericd\\appdata\\roaming\\python\\python37\\site-packages (from cycler>=0.10->matplotlib>=3.0.0->pyclustering) (1.15.0)\n",
      "Building wheels for collected packages: pyclustering\n",
      "  Building wheel for pyclustering (setup.py): started\n",
      "  Building wheel for pyclustering (setup.py): finished with status 'done'\n",
      "  Created wheel for pyclustering: filename=pyclustering-0.10.1.2-py3-none-any.whl size=2395108 sha256=d633405596b48174ca4c35a902f4b350539ece94d62f78c77fe94f5f1c263c8e\n",
      "  Stored in directory: c:\\users\\ericd\\appdata\\local\\pip\\cache\\wheels\\ea\\87\\6b\\1e0568b5ba9dc6518a25338bae90bd8392f35206bb90bb10f1\n",
      "Successfully built pyclustering\n",
      "Installing collected packages: pyclustering\n",
      "Successfully installed pyclustering-0.10.1.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyclustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyclustering.cluster.kmeans import kmeans\n",
    "from pyclustering.utils.metric import type_metric, distance_metric\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from k_means_constrained import KMeansConstrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "df = pd.read_csv('dgn_raw_data.csv')\n",
    "\n",
    "# Add very small random number to Rating\n",
    "df['target']=df['Rating'].apply(lambda x: x+random.random()/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions for Each UID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique IDs\n",
    "ids = df.UID.unique()\n",
    "\n",
    "# Run linear regressions for each UID\n",
    "op = pd.DataFrame\n",
    "intercept = []\n",
    "coefficients=[]\n",
    "UID = []\n",
    "for p in ids:\n",
    "    df_i = df[df.UID == p]              # Create dataframe for current user id\n",
    "    X = df_i.filter(regex='^[a-zA-Z][0-9]')  # df input variables only\n",
    "    y = df_i['target']                  # Series of target variable\n",
    "    reg = LinearRegression().fit(X, y)  # Fit linear regression\n",
    "    reg.score(X, y)                     # Score regression model\n",
    "    unique_id=df_i['UID'].unique()      # Saves current user id\n",
    "    const = reg.intercept_              # Save intercept of the regression model\n",
    "    coef = reg.coef_                    # Coefficients of regression model\n",
    "    UID.append(unique_id)               # Append current user id\n",
    "    intercept.append(const)             # Append current intercept\n",
    "    coefficients.append(coef)           # Append current regression coefficients\n",
    "\n",
    "# Convert newly created lists into dataframes\n",
    "intercep_new = pd.DataFrame(intercept)\n",
    "coefficients_new = pd.DataFrame(coefficients)\n",
    "UID_new = pd.DataFrame(UID)\n",
    "\n",
    "# Get columns names\n",
    "colNames = df.drop(['Rating', 'target',], axis=1).columns\n",
    "colNames = colNames.insert(1, 'Const')\n",
    "colNames\n",
    "\n",
    "# Concatenate the new dataframes and add column names\n",
    "op = pd.concat([UID_new,intercep_new, coefficients_new], axis=1)\n",
    "op.columns = colNames\n",
    "\n",
    "# Save only regression coefficients for clustering\n",
    "scores = op.drop(['UID','Const'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_dist(x, y):\n",
    "    r = stats.pearsonr(x, y)[0]\n",
    "    return (1 - r) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for n in range(2, 3): # max_clusters+1):\n",
    "    \n",
    "    # change your df to numpy arr\n",
    "    sample = scores.to_numpy()\n",
    "    \n",
    "    # define a custom metric\n",
    "    metric = distance_metric(type_metric.USER_DEFINED, func=pearson_dist)\n",
    "    \n",
    "    # carry out a km++ init\n",
    "    initial_centers = kmeans_plusplus_initializer(sample, 3, random_state=123).initialize()\n",
    "    \n",
    "    # execute kmeans\n",
    "    kmeans_instance = kmeans(sample, initial_centers, metric=metric)\n",
    "    \n",
    "    # run cluster analysis\n",
    "    kmeans_instance.process()\n",
    "    \n",
    "    # get clusters\n",
    "    clusters = kmeans_instance.get_clusters()\n",
    "\n",
    "# Now I need to label each observation with the assigned cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericd\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\ericd\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\ericd\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Quick and dirty way to get cluster assignments in order\n",
    "df1 = scores.iloc[clusters[0],:]\n",
    "df1['cluster'] = 1\n",
    "df2 = scores.iloc[clusters[1],:]\n",
    "df2['cluster'] = 2\n",
    "df3 = scores.iloc[clusters[2],:]\n",
    "df3['cluster'] = 3\n",
    "# df4 = scores.iloc[clusters[3],:]\n",
    "# df4['cluster'] = 4\n",
    "# df5 = scores.iloc[clusters[4],:]\n",
    "# df5['cluster'] = 5\n",
    "df7 = pd.concat([df1, df2, df3]) #, df4, df5])\n",
    "df7.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.751866</td>\n",
       "      <td>-12.870270</td>\n",
       "      <td>0.538372</td>\n",
       "      <td>14.688970</td>\n",
       "      <td>44.408728</td>\n",
       "      <td>4.669741</td>\n",
       "      <td>-2.317787</td>\n",
       "      <td>-21.845110</td>\n",
       "      <td>-1.530777</td>\n",
       "      <td>-22.509026</td>\n",
       "      <td>-57.125334</td>\n",
       "      <td>-74.990726</td>\n",
       "      <td>26.501062</td>\n",
       "      <td>41.034938</td>\n",
       "      <td>51.224244</td>\n",
       "      <td>11.255726</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.040325</td>\n",
       "      <td>90.018320</td>\n",
       "      <td>74.390796</td>\n",
       "      <td>66.397211</td>\n",
       "      <td>1.826316</td>\n",
       "      <td>-33.025963</td>\n",
       "      <td>3.179185</td>\n",
       "      <td>-15.475670</td>\n",
       "      <td>-7.107935</td>\n",
       "      <td>19.450121</td>\n",
       "      <td>-44.596869</td>\n",
       "      <td>-10.818940</td>\n",
       "      <td>89.524432</td>\n",
       "      <td>83.429265</td>\n",
       "      <td>48.008631</td>\n",
       "      <td>93.441295</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.702993</td>\n",
       "      <td>64.170224</td>\n",
       "      <td>15.395325</td>\n",
       "      <td>18.860226</td>\n",
       "      <td>-4.502263</td>\n",
       "      <td>4.676741</td>\n",
       "      <td>-57.286873</td>\n",
       "      <td>-20.365999</td>\n",
       "      <td>27.851003</td>\n",
       "      <td>-50.189844</td>\n",
       "      <td>-59.491059</td>\n",
       "      <td>-38.873911</td>\n",
       "      <td>6.686825</td>\n",
       "      <td>-59.898166</td>\n",
       "      <td>-52.237304</td>\n",
       "      <td>-31.525426</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.414670</td>\n",
       "      <td>-7.567968</td>\n",
       "      <td>-37.699594</td>\n",
       "      <td>-8.989126</td>\n",
       "      <td>-13.425293</td>\n",
       "      <td>-79.308879</td>\n",
       "      <td>-37.854074</td>\n",
       "      <td>-46.698947</td>\n",
       "      <td>-72.715060</td>\n",
       "      <td>-25.546613</td>\n",
       "      <td>-93.735573</td>\n",
       "      <td>-90.750008</td>\n",
       "      <td>-20.306692</td>\n",
       "      <td>1.257164</td>\n",
       "      <td>-15.607941</td>\n",
       "      <td>-32.249962</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-11.427231</td>\n",
       "      <td>-77.014921</td>\n",
       "      <td>-54.392280</td>\n",
       "      <td>-72.349218</td>\n",
       "      <td>21.059324</td>\n",
       "      <td>73.217126</td>\n",
       "      <td>19.827870</td>\n",
       "      <td>18.753911</td>\n",
       "      <td>5.367141</td>\n",
       "      <td>-83.649367</td>\n",
       "      <td>-32.425178</td>\n",
       "      <td>-1.291034</td>\n",
       "      <td>-27.986170</td>\n",
       "      <td>-42.094248</td>\n",
       "      <td>4.238071</td>\n",
       "      <td>-19.124844</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-66.469901</td>\n",
       "      <td>76.288105</td>\n",
       "      <td>-19.909205</td>\n",
       "      <td>13.399902</td>\n",
       "      <td>-94.325731</td>\n",
       "      <td>-53.427247</td>\n",
       "      <td>-58.926777</td>\n",
       "      <td>-11.313184</td>\n",
       "      <td>-117.962833</td>\n",
       "      <td>-36.644532</td>\n",
       "      <td>-121.168342</td>\n",
       "      <td>-41.603049</td>\n",
       "      <td>2.065587</td>\n",
       "      <td>23.887201</td>\n",
       "      <td>-29.464238</td>\n",
       "      <td>-23.558038</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56.304434</td>\n",
       "      <td>109.304238</td>\n",
       "      <td>71.408837</td>\n",
       "      <td>95.372400</td>\n",
       "      <td>94.960828</td>\n",
       "      <td>95.898454</td>\n",
       "      <td>45.689436</td>\n",
       "      <td>70.750313</td>\n",
       "      <td>-45.140808</td>\n",
       "      <td>-26.413627</td>\n",
       "      <td>37.647850</td>\n",
       "      <td>14.881783</td>\n",
       "      <td>63.037887</td>\n",
       "      <td>89.801714</td>\n",
       "      <td>39.553334</td>\n",
       "      <td>108.352659</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>103.481246</td>\n",
       "      <td>55.044443</td>\n",
       "      <td>84.413384</td>\n",
       "      <td>124.128787</td>\n",
       "      <td>57.783178</td>\n",
       "      <td>-10.816578</td>\n",
       "      <td>70.379344</td>\n",
       "      <td>86.723114</td>\n",
       "      <td>62.679910</td>\n",
       "      <td>62.826086</td>\n",
       "      <td>59.127696</td>\n",
       "      <td>91.514311</td>\n",
       "      <td>-12.181992</td>\n",
       "      <td>-19.315085</td>\n",
       "      <td>-46.141581</td>\n",
       "      <td>-54.488294</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.423049</td>\n",
       "      <td>54.939540</td>\n",
       "      <td>39.764011</td>\n",
       "      <td>13.098282</td>\n",
       "      <td>-22.677849</td>\n",
       "      <td>-41.948100</td>\n",
       "      <td>40.918731</td>\n",
       "      <td>-0.009657</td>\n",
       "      <td>83.274812</td>\n",
       "      <td>7.563807</td>\n",
       "      <td>30.345692</td>\n",
       "      <td>50.848630</td>\n",
       "      <td>22.252235</td>\n",
       "      <td>-62.461443</td>\n",
       "      <td>12.962632</td>\n",
       "      <td>-31.969422</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.904128</td>\n",
       "      <td>-40.704791</td>\n",
       "      <td>-36.932896</td>\n",
       "      <td>-22.913089</td>\n",
       "      <td>-40.262158</td>\n",
       "      <td>-35.133350</td>\n",
       "      <td>-75.701752</td>\n",
       "      <td>-53.292250</td>\n",
       "      <td>-45.851876</td>\n",
       "      <td>22.132605</td>\n",
       "      <td>-48.617459</td>\n",
       "      <td>-14.569965</td>\n",
       "      <td>26.592945</td>\n",
       "      <td>38.940621</td>\n",
       "      <td>43.088414</td>\n",
       "      <td>-0.460638</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           A1          A2         A3          A4         B1         B2  \\\n",
       "0   21.751866  -12.870270   0.538372   14.688970  44.408728   4.669741   \n",
       "1   86.040325   90.018320  74.390796   66.397211   1.826316 -33.025963   \n",
       "2   67.702993   64.170224  15.395325   18.860226  -4.502263   4.676741   \n",
       "3   -3.414670   -7.567968 -37.699594   -8.989126 -13.425293 -79.308879   \n",
       "4  -11.427231  -77.014921 -54.392280  -72.349218  21.059324  73.217126   \n",
       "5  -66.469901   76.288105 -19.909205   13.399902 -94.325731 -53.427247   \n",
       "6   56.304434  109.304238  71.408837   95.372400  94.960828  95.898454   \n",
       "7  103.481246   55.044443  84.413384  124.128787  57.783178 -10.816578   \n",
       "8   10.423049   54.939540  39.764011   13.098282 -22.677849 -41.948100   \n",
       "9    0.904128  -40.704791 -36.932896  -22.913089 -40.262158 -35.133350   \n",
       "\n",
       "          B3         B4          C1         C2          C3         C4  \\\n",
       "0  -2.317787 -21.845110   -1.530777 -22.509026  -57.125334 -74.990726   \n",
       "1   3.179185 -15.475670   -7.107935  19.450121  -44.596869 -10.818940   \n",
       "2 -57.286873 -20.365999   27.851003 -50.189844  -59.491059 -38.873911   \n",
       "3 -37.854074 -46.698947  -72.715060 -25.546613  -93.735573 -90.750008   \n",
       "4  19.827870  18.753911    5.367141 -83.649367  -32.425178  -1.291034   \n",
       "5 -58.926777 -11.313184 -117.962833 -36.644532 -121.168342 -41.603049   \n",
       "6  45.689436  70.750313  -45.140808 -26.413627   37.647850  14.881783   \n",
       "7  70.379344  86.723114   62.679910  62.826086   59.127696  91.514311   \n",
       "8  40.918731  -0.009657   83.274812   7.563807   30.345692  50.848630   \n",
       "9 -75.701752 -53.292250  -45.851876  22.132605  -48.617459 -14.569965   \n",
       "\n",
       "          D1         D2         D3          D4  cluster  \n",
       "0  26.501062  41.034938  51.224244   11.255726        3  \n",
       "1  89.524432  83.429265  48.008631   93.441295        3  \n",
       "2   6.686825 -59.898166 -52.237304  -31.525426        3  \n",
       "3 -20.306692   1.257164 -15.607941  -32.249962        3  \n",
       "4 -27.986170 -42.094248   4.238071  -19.124844        1  \n",
       "5   2.065587  23.887201 -29.464238  -23.558038        3  \n",
       "6  63.037887  89.801714  39.553334  108.352659        3  \n",
       "7 -12.181992 -19.315085 -46.141581  -54.488294        3  \n",
       "8  22.252235 -62.461443  12.962632  -31.969422        2  \n",
       "9  26.592945  38.940621  43.088414   -0.460638        2  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005619558487412072"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(scores, df7['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "murph = pd.DataFrame(distance_matrix(scores, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054572507092937934"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(murph, df3['cluster'], metric='precomputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "murph = pd.DataFrame(distance.cdist(scores, scores, 'correlation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24159138620376253"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(murph, df3['cluster'], metric='precomputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.22044605e-16, 4.75077700e-01, 8.32905828e-01, ...,\n",
       "        1.38928296e+00, 1.49716120e+00, 1.36233799e+00],\n",
       "       [4.75077700e-01, 0.00000000e+00, 6.26187537e-01, ...,\n",
       "        9.10971423e-01, 1.17294837e+00, 6.58036963e-01],\n",
       "       [8.32905828e-01, 6.26187537e-01, 0.00000000e+00, ...,\n",
       "        1.03625431e+00, 8.88826031e-01, 7.58852073e-01],\n",
       "       ...,\n",
       "       [1.38928296e+00, 9.10971423e-01, 1.03625431e+00, ...,\n",
       "        0.00000000e+00, 3.88764696e-01, 2.60716426e-01],\n",
       "       [1.49716120e+00, 1.17294837e+00, 8.88826031e-01, ...,\n",
       "        3.88764696e-01, 0.00000000e+00, 4.30715093e-01],\n",
       "       [1.36233799e+00, 6.58036963e-01, 7.58852073e-01, ...,\n",
       "        2.60716426e-01, 4.30715093e-01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cdist(scores, scores, 'correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster on Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell is testing constrained clustering compared to others.\n",
    "#### Skip this cell and run the next one to get the Excel output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1], dtype=int64), array([80, 20], dtype=int64))\n",
      "Silhouette Score: -0.017060443699019583 \n",
      "\n",
      "(array([0, 1, 2], dtype=int64), array([49, 20, 31], dtype=int64))\n",
      "Silhouette Score: -0.019000365194238606 \n",
      "\n",
      "(array([0, 1, 2, 3], dtype=int64), array([23, 20, 31, 26], dtype=int64))\n",
      "Silhouette Score: -0.03026897463440398 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pearson_affinity(M):\n",
    "   return 1 - np.array([[stats.pearsonr(a,b)[0] for a in M] for b in M])\n",
    "\n",
    "pears_cluster_2 = AgglomerativeClustering(n_clusters=2, linkage='complete', affinity=pearson_affinity)\n",
    "pears_cluster_3 = AgglomerativeClustering(n_clusters=3, linkage='complete', affinity=pearson_affinity)\n",
    "pears_cluster_4 = AgglomerativeClustering(n_clusters=4, linkage='complete', affinity=pearson_affinity)\n",
    "\n",
    "pears_cluster_2.fit(scores)\n",
    "pears_cluster_3.fit(scores)\n",
    "pears_cluster_4.fit(scores)\n",
    "\n",
    "pears_cluster_2_clus = pears_cluster_2.fit_predict(scores)\n",
    "pears_cluster_3_clus = pears_cluster_3.fit_predict(scores)\n",
    "pears_cluster_4_clus = pears_cluster_4.fit_predict(scores)\n",
    "\n",
    "op['pearson_2_clus'] = pears_cluster_2_clus\n",
    "op['pearson_3_clus'] = pears_cluster_3_clus\n",
    "op['pearson_4_clus'] = pears_cluster_4_clus\n",
    "\n",
    "\n",
    "print(np.unique(op['pearson_2_clus'], return_counts=True))\n",
    "print(\"Silhouette Score:\", silhouette_score(scores,op['pearson_2_clus']), '\\n')\n",
    "\n",
    "print(np.unique(op['pearson_3_clus'], return_counts=True))\n",
    "print(\"Silhouette Score:\", silhouette_score(scores,op['pearson_3_clus']), '\\n')\n",
    "\n",
    "print(np.unique(op['pearson_4_clus'], return_counts=True))\n",
    "print(\"Silhouette Score:\", silhouette_score(scores,op['pearson_4_clus']), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>Const</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>...</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>pearson_2_clus</th>\n",
       "      <th>pearson_3_clus</th>\n",
       "      <th>pearson_4_clus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>65.557239</td>\n",
       "      <td>21.751485</td>\n",
       "      <td>-12.870657</td>\n",
       "      <td>0.538530</td>\n",
       "      <td>14.688945</td>\n",
       "      <td>44.407940</td>\n",
       "      <td>4.669149</td>\n",
       "      <td>-2.318242</td>\n",
       "      <td>-21.845643</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.509201</td>\n",
       "      <td>-57.124935</td>\n",
       "      <td>-74.990449</td>\n",
       "      <td>26.500964</td>\n",
       "      <td>41.034439</td>\n",
       "      <td>51.224021</td>\n",
       "      <td>11.255683</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-59.461485</td>\n",
       "      <td>86.040298</td>\n",
       "      <td>90.018173</td>\n",
       "      <td>74.390966</td>\n",
       "      <td>66.397653</td>\n",
       "      <td>1.826554</td>\n",
       "      <td>-33.025940</td>\n",
       "      <td>3.179548</td>\n",
       "      <td>-15.475066</td>\n",
       "      <td>...</td>\n",
       "      <td>19.449666</td>\n",
       "      <td>-44.596710</td>\n",
       "      <td>-10.818794</td>\n",
       "      <td>89.524184</td>\n",
       "      <td>83.429701</td>\n",
       "      <td>48.009415</td>\n",
       "      <td>93.441843</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>91.635359</td>\n",
       "      <td>67.703222</td>\n",
       "      <td>64.170551</td>\n",
       "      <td>15.395709</td>\n",
       "      <td>18.860242</td>\n",
       "      <td>-4.501816</td>\n",
       "      <td>4.677236</td>\n",
       "      <td>-57.286783</td>\n",
       "      <td>-20.366019</td>\n",
       "      <td>...</td>\n",
       "      <td>-50.189533</td>\n",
       "      <td>-59.491276</td>\n",
       "      <td>-38.874193</td>\n",
       "      <td>6.687683</td>\n",
       "      <td>-59.897168</td>\n",
       "      <td>-52.237096</td>\n",
       "      <td>-31.524399</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>168.803546</td>\n",
       "      <td>-3.414435</td>\n",
       "      <td>-7.567551</td>\n",
       "      <td>-37.699759</td>\n",
       "      <td>-8.988798</td>\n",
       "      <td>-13.425835</td>\n",
       "      <td>-79.308729</td>\n",
       "      <td>-37.853967</td>\n",
       "      <td>-46.698624</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.546170</td>\n",
       "      <td>-93.735683</td>\n",
       "      <td>-90.750206</td>\n",
       "      <td>-20.307225</td>\n",
       "      <td>1.256831</td>\n",
       "      <td>-15.608174</td>\n",
       "      <td>-32.250176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>105.171557</td>\n",
       "      <td>-11.427539</td>\n",
       "      <td>-77.014891</td>\n",
       "      <td>-54.392512</td>\n",
       "      <td>-72.349070</td>\n",
       "      <td>21.059202</td>\n",
       "      <td>73.216677</td>\n",
       "      <td>19.827303</td>\n",
       "      <td>18.753513</td>\n",
       "      <td>...</td>\n",
       "      <td>-83.649564</td>\n",
       "      <td>-32.425404</td>\n",
       "      <td>-1.291158</td>\n",
       "      <td>-27.985901</td>\n",
       "      <td>-42.093885</td>\n",
       "      <td>4.237622</td>\n",
       "      <td>-19.124722</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID       Const         A1         A2         A3         A4         B1  \\\n",
       "0    1   65.557239  21.751485 -12.870657   0.538530  14.688945  44.407940   \n",
       "1    2  -59.461485  86.040298  90.018173  74.390966  66.397653   1.826554   \n",
       "2    3   91.635359  67.703222  64.170551  15.395709  18.860242  -4.501816   \n",
       "3    4  168.803546  -3.414435  -7.567551 -37.699759  -8.988798 -13.425835   \n",
       "4    5  105.171557 -11.427539 -77.014891 -54.392512 -72.349070  21.059202   \n",
       "\n",
       "          B2         B3         B4  ...         C2         C3         C4  \\\n",
       "0   4.669149  -2.318242 -21.845643  ... -22.509201 -57.124935 -74.990449   \n",
       "1 -33.025940   3.179548 -15.475066  ...  19.449666 -44.596710 -10.818794   \n",
       "2   4.677236 -57.286783 -20.366019  ... -50.189533 -59.491276 -38.874193   \n",
       "3 -79.308729 -37.853967 -46.698624  ... -25.546170 -93.735683 -90.750206   \n",
       "4  73.216677  19.827303  18.753513  ... -83.649564 -32.425404  -1.291158   \n",
       "\n",
       "          D1         D2         D3         D4  pearson_2_clus  pearson_3_clus  \\\n",
       "0  26.500964  41.034439  51.224021  11.255683               0               0   \n",
       "1  89.524184  83.429701  48.009415  93.441843               0               0   \n",
       "2   6.687683 -59.897168 -52.237096 -31.524399               0               2   \n",
       "3 -20.307225   1.256831 -15.608174 -32.250176               0               0   \n",
       "4 -27.985901 -42.093885   4.237622 -19.124722               1               1   \n",
       "\n",
       "   pearson_4_clus  \n",
       "0               0  \n",
       "1               0  \n",
       "2               2  \n",
       "3               0  \n",
       "4               1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "constrained_stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([25, 75], dtype=int64))\n",
      "2 Cluster Solutions\n",
      "[['Hierarchical', 2, 0.37139533642783606, array([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], dtype=int64)], ['kMeans', 2, 0.35855513866493616, array([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1])], ['Constrained', 2, 0.34150130457438893, array([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1])], ['kMedclf.cluster_centers_oids', 2, 0.024307295579601474, array([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
      "       0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
      "       0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], dtype=int64)]] \n",
      "\n",
      "Cluster Constraints:\n",
      "             Minimum: 15\n",
      "             Maximum: 75\n",
      "\n",
      "(array([0, 1, 2]), array([69, 15, 16], dtype=int64))\n",
      "3 Cluster Solutions\n",
      "[['kMeans', 3, 0.3925791933597397, array([1, 0, 0, 2, 1, 2, 0, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1,\n",
      "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1])], ['Hierarchical', 3, 0.39098630302507514, array([0, 2, 0, 1, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], dtype=int64)], ['Constrained', 3, 0.3574907456996062, array([1, 1, 2, 2, 0, 2, 1, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0, 1,\n",
      "       1, 1, 2, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0,\n",
      "       0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0])], ['kMedclf.cluster_centers_oids', 3, 0.12329040973652522, array([0, 0, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0,\n",
      "       0, 0, 2, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 2, 0, 1, 1, 2, 1, 1, 1, 2,\n",
      "       1, 2, 2, 0, 1, 1, 2, 0, 0, 2, 2, 2, 2, 1, 2, 2, 1, 2, 0, 0, 0, 2,\n",
      "       2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 0, 1, 2, 2, 1, 1,\n",
      "       1, 2, 1, 1, 0, 0, 0, 1, 2, 2, 2, 1], dtype=int64)]] \n",
      "\n",
      "Cluster Constraints:\n",
      "             Minimum: 15\n",
      "             Maximum: 75\n",
      "\n",
      "(array([0, 1, 2, 3]), array([55, 15, 15, 15], dtype=int64))\n",
      "4 Cluster Solutions\n",
      "[['kMeans', 4, 0.38968441314787766, array([3, 1, 3, 0, 2, 0, 1, 1, 3, 0, 2, 3, 3, 3, 1, 2, 2, 3, 2, 0, 3, 3,\n",
      "       1, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "       3, 2, 3, 3, 3, 3, 0, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3,\n",
      "       3, 3, 2, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 1, 3, 3, 0, 3, 3,\n",
      "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3])], ['Hierarchical', 4, 0.3631752978990081, array([1, 2, 1, 0, 3, 0, 2, 1, 1, 0, 3, 3, 3, 3, 1, 3, 3, 1, 3, 0, 1, 1,\n",
      "       1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 3, 1, 1,\n",
      "       3, 1, 3, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 0, 1], dtype=int64)], ['Constrained', 4, 0.3419103765025841, array([0, 2, 3, 3, 1, 3, 2, 2, 2, 3, 1, 1, 1, 1, 2, 1, 1, 0, 3, 3, 1, 2,\n",
      "       2, 2, 3, 0, 1, 0, 0, 2, 2, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 3, 0, 0, 0, 0, 3, 0, 2, 0, 3, 3, 0, 0, 0, 1, 0, 3, 0, 1, 0, 0,\n",
      "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 3, 0])], ['kMedclf.cluster_centers_oids', 4, 0.032449285117340035, array([3, 3, 0, 2, 2, 2, 3, 0, 0, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3,\n",
      "       0, 3, 2, 1, 3, 1, 1, 0, 3, 1, 1, 1, 3, 2, 3, 1, 1, 2, 1, 1, 1, 2,\n",
      "       0, 2, 2, 3, 0, 1, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 1, 2, 3, 2, 3, 2,\n",
      "       2, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 0, 2, 1, 0, 1, 0, 1, 2, 2, 1, 1,\n",
      "       1, 2, 1, 0, 3, 0, 3, 1, 2, 3, 2, 0], dtype=int64)]] \n",
      "\n",
      "Cluster Constraints:\n",
      "             Minimum: 15\n",
      "             Maximum: 75\n",
      "\n",
      "(array([0, 1, 2, 3, 4]), array([15, 15, 40, 15, 15], dtype=int64))\n",
      "5 Cluster Solutions\n",
      "[['kMeans', 5, 0.3967079375928836, array([2, 1, 4, 3, 0, 4, 1, 1, 2, 3, 0, 2, 2, 2, 1, 0, 0, 2, 3, 3, 2, 2,\n",
      "       4, 1, 4, 2, 2, 2, 2, 4, 1, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2,\n",
      "       2, 2, 0, 2, 3, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 3, 2])], ['Hierarchical', 5, 0.37521692286479563, array([0, 2, 0, 4, 1, 4, 2, 0, 0, 3, 1, 1, 1, 1, 0, 1, 1, 0, 1, 3, 0, 0,\n",
      "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0,\n",
      "       1, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0], dtype=int64)], ['Constrained', 5, 0.2686028388374959, array([4, 1, 4, 4, 0, 4, 1, 1, 1, 3, 0, 3, 0, 0, 1, 0, 0, 3, 0, 3, 0, 1,\n",
      "       4, 1, 4, 2, 1, 2, 2, 4, 1, 2, 2, 2, 1, 3, 2, 2, 2, 4, 2, 2, 2, 3,\n",
      "       2, 3, 2, 3, 4, 2, 3, 0, 1, 2, 3, 3, 4, 0, 2, 0, 2, 4, 1, 0, 4, 4,\n",
      "       0, 2, 0, 2, 3, 2, 2, 2, 2, 2, 2, 2, 4, 2, 1, 2, 4, 2, 2, 3, 2, 2,\n",
      "       2, 2, 2, 2, 1, 1, 2, 2, 0, 3, 3, 2])], ['kMedclf.cluster_centers_oids', 5, 0.043690104449045426, array([4, 4, 0, 2, 1, 2, 4, 4, 4, 2, 1, 1, 1, 1, 4, 1, 1, 0, 2, 2, 1, 4,\n",
      "       4, 4, 2, 3, 4, 3, 3, 0, 4, 3, 3, 3, 4, 2, 4, 3, 3, 1, 3, 3, 3, 2,\n",
      "       0, 2, 1, 4, 0, 3, 2, 1, 4, 2, 2, 2, 1, 1, 2, 0, 3, 2, 4, 1, 4, 1,\n",
      "       2, 0, 2, 3, 1, 3, 3, 3, 3, 3, 3, 0, 2, 3, 4, 3, 4, 3, 1, 2, 3, 3,\n",
      "       3, 2, 3, 0, 4, 0, 4, 3, 1, 0, 2, 3], dtype=int64)]] \n",
      "\n",
      "Cluster Constraints:\n",
      "             Minimum: 15\n",
      "             Maximum: 75\n",
      "\n",
      "(array([0, 1, 2, 3, 4, 5]), array([25, 15, 15, 15, 15, 15], dtype=int64))\n",
      "6 Cluster Solutions\n",
      "[['kMeans', 6, 0.396039427237966, array([1, 0, 3, 3, 4, 3, 0, 0, 1, 5, 4, 1, 1, 1, 0, 4, 4, 1, 1, 5, 1, 1,\n",
      "       1, 0, 3, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 5, 1, 1, 1, 1, 5, 1, 1, 1, 5, 5, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1,\n",
      "       1, 1, 4, 1, 5, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 5, 1])], ['Hierarchical', 6, 0.3756248702204344, array([1, 5, 1, 4, 0, 4, 5, 2, 1, 3, 0, 0, 0, 0, 2, 0, 0, 1, 0, 3, 1, 2,\n",
      "       1, 2, 0, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 4, 1, 0, 1, 1,\n",
      "       0, 1, 0, 1, 3, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 5, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 3, 1], dtype=int64)], ['kMedclf.cluster_centers_oids', 6, 0.04198952135458508, array([2, 4, 2, 2, 5, 2, 4, 4, 1, 1, 2, 1, 5, 2, 4, 5, 5, 1, 2, 1, 2, 1,\n",
      "       2, 4, 2, 3, 4, 3, 3, 2, 4, 3, 3, 3, 4, 1, 4, 3, 3, 2, 3, 3, 3, 3,\n",
      "       0, 1, 5, 1, 0, 3, 1, 0, 4, 3, 1, 1, 2, 3, 5, 5, 3, 2, 4, 5, 2, 2,\n",
      "       5, 5, 1, 3, 1, 3, 3, 3, 3, 3, 3, 0, 2, 3, 4, 3, 2, 3, 3, 1, 3, 3,\n",
      "       3, 5, 3, 0, 4, 0, 4, 3, 5, 1, 1, 0], dtype=int64)], ['Constrained', 6, -0.0054816217745239215, array([1, 5, 1, 1, 3, 1, 5, 5, 5, 2, 3, 2, 3, 3, 5, 3, 3, 2, 3, 2, 3, 5,\n",
      "       1, 5, 1, 0, 5, 0, 4, 1, 5, 0, 0, 0, 5, 2, 0, 4, 0, 1, 0, 0, 0, 2,\n",
      "       4, 2, 3, 2, 4, 0, 2, 5, 5, 4, 2, 2, 1, 3, 4, 3, 0, 1, 5, 3, 1, 1,\n",
      "       3, 4, 3, 0, 2, 0, 0, 0, 4, 4, 0, 4, 1, 4, 5, 4, 1, 0, 4, 2, 0, 4,\n",
      "       0, 4, 0, 0, 5, 1, 0, 0, 3, 2, 2, 0])]] \n",
      "\n",
      "Cluster Constraints:\n",
      "             Minimum: 15\n",
      "             Maximum: 75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Maximum number of clusters\n",
    "max_clusters = 6\n",
    "\n",
    "# Total rows in dataframe\n",
    "total_rows = len(scores)\n",
    "\n",
    "for n in range(2, max_clusters+1):\n",
    "    \n",
    "    constrained_iteration = []\n",
    "    \n",
    "    sw = []\n",
    "    \n",
    "    # Set max/min sizes for constrained clustering\n",
    "#     size_min = np.floor((len(scores)/n)/10)*10\n",
    "    size_min = 15\n",
    "#     size_max = np.ceil((len(scores)/n)/10)*10\n",
    "    size_max = 75\n",
    "    \n",
    "    # Create clustering objects\n",
    "    cls1 = KMeans(n_clusters=n, random_state=0)\n",
    "    cls2 = KMedoids(n_clusters=n, random_state=0)\n",
    "    cls3 = AgglomerativeClustering(n_clusters=n,\n",
    "                                   affinity='euclidean',\n",
    "                                   linkage='ward')\n",
    "    cls4 = KMeansConstrained(n_clusters=n,\n",
    "                             size_min=size_min,\n",
    "                             size_max=size_max,\n",
    "                             random_state=0)\n",
    "    \n",
    "    # Agglomerative clustering: if linkage=ward, affinity must be Euclidean\n",
    "    cls_algs = [['kMeans', cls1],\n",
    "                ['kMedclf.cluster_centers_oids', cls2],\n",
    "                ['Hierarchical', cls3],\n",
    "                ['Constrained', cls4]]\n",
    "    \n",
    "    # Fit and score clustering solutions for i clusters w/ each algorithm\n",
    "    for cls in cls_algs:\n",
    "        \n",
    "        # Fit the model to the factor analysis scores\n",
    "        cls[1].fit(scores)\n",
    "        \n",
    "        # List of assigned clusters\n",
    "        clusters = cls[1].fit_predict(scores)\n",
    "        \n",
    "        # Silhouette scores for each solution\n",
    "        silhouette_avg = silhouette_score(scores,clusters)\n",
    "        \n",
    "        # Store solution info\n",
    "        algorithm = cls[0]\n",
    "        n_stats = [algorithm, n, silhouette_avg, clusters]\n",
    "        sw.append(n_stats)\n",
    "        \n",
    "        if cls[0] == \"Constrained\":\n",
    "            cluster_bases = np.unique(clusters, return_counts=True)\n",
    "            print(cluster_bases)\n",
    "            constrained_iteration.append(size_min)\n",
    "            constrained_iteration.append(size_max)\n",
    "            constrained_iteration.append(cluster_bases[1])\n",
    "            constrained_iteration.append(silhouette_avg)\n",
    "            constrained_stats.append(constrained_iteration)\n",
    "\n",
    "    # Reorder cluster lists by descending silhouette scores.\n",
    "    # Clusters in first element should be assigned to training data.\n",
    "    sw = sorted(sw, key=itemgetter(2), reverse=True)\n",
    "    op[f'Optimal {sw[0][1]} cluster solution ({sw[0][0]})'] = sw[0][3] + 1\n",
    "    \n",
    "    print(n, \"Cluster Solutions\")\n",
    "    print(sw,\"\\n\")\n",
    "    print(f\"\"\"Cluster Constraints:\n",
    "             Minimum: {size_min}\n",
    "             Maximum: {size_max}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>bases</th>\n",
       "      <th>silhouette_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>[15, 85]</td>\n",
       "      <td>0.358555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>[6, 19, 75]</td>\n",
       "      <td>0.371596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>[13, 6, 8, 73]</td>\n",
       "      <td>0.389684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>[74, 5, 10, 6, 5]</td>\n",
       "      <td>0.410444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>[8, 66, 5, 6, 6, 9]</td>\n",
       "      <td>0.396627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>[15, 85]</td>\n",
       "      <td>0.358555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>[75, 10, 15]</td>\n",
       "      <td>0.373687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>[10, 68, 12, 10]</td>\n",
       "      <td>0.391894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>[10, 10, 60, 10, 10]</td>\n",
       "      <td>0.385096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>[10, 50, 10, 10, 10, 10]</td>\n",
       "      <td>0.345982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "      <td>[15, 85]</td>\n",
       "      <td>0.358555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "      <td>[75, 10, 15]</td>\n",
       "      <td>0.373687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "      <td>[10, 68, 12, 10]</td>\n",
       "      <td>0.391894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "      <td>[10, 10, 60, 10, 10]</td>\n",
       "      <td>0.385096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "      <td>[10, 50, 10, 10, 10, 10]</td>\n",
       "      <td>0.345982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>80</td>\n",
       "      <td>[78, 22]</td>\n",
       "      <td>0.356513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>80</td>\n",
       "      <td>[75, 10, 15]</td>\n",
       "      <td>0.373687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>80</td>\n",
       "      <td>[10, 68, 12, 10]</td>\n",
       "      <td>0.391894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>80</td>\n",
       "      <td>[10, 10, 60, 10, 10]</td>\n",
       "      <td>0.385096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>80</td>\n",
       "      <td>[10, 50, 10, 10, 10, 10]</td>\n",
       "      <td>0.345982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15</td>\n",
       "      <td>80</td>\n",
       "      <td>[78, 22]</td>\n",
       "      <td>0.356513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15</td>\n",
       "      <td>80</td>\n",
       "      <td>[69, 15, 16]</td>\n",
       "      <td>0.357491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15</td>\n",
       "      <td>80</td>\n",
       "      <td>[55, 15, 15, 15]</td>\n",
       "      <td>0.341910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15</td>\n",
       "      <td>80</td>\n",
       "      <td>[15, 15, 40, 15, 15]</td>\n",
       "      <td>0.268603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15</td>\n",
       "      <td>80</td>\n",
       "      <td>[25, 15, 15, 15, 15, 15]</td>\n",
       "      <td>-0.005482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15</td>\n",
       "      <td>75</td>\n",
       "      <td>[25, 75]</td>\n",
       "      <td>0.341501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15</td>\n",
       "      <td>75</td>\n",
       "      <td>[69, 15, 16]</td>\n",
       "      <td>0.357491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15</td>\n",
       "      <td>75</td>\n",
       "      <td>[55, 15, 15, 15]</td>\n",
       "      <td>0.341910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15</td>\n",
       "      <td>75</td>\n",
       "      <td>[15, 15, 40, 15, 15]</td>\n",
       "      <td>0.268603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15</td>\n",
       "      <td>75</td>\n",
       "      <td>[25, 15, 15, 15, 15, 15]</td>\n",
       "      <td>-0.005482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    min  max                     bases  silhouette_score\n",
       "0     5   90                  [15, 85]          0.358555\n",
       "1     5   90               [6, 19, 75]          0.371596\n",
       "2     5   90            [13, 6, 8, 73]          0.389684\n",
       "3     5   90         [74, 5, 10, 6, 5]          0.410444\n",
       "4     5   90       [8, 66, 5, 6, 6, 9]          0.396627\n",
       "5    10   90                  [15, 85]          0.358555\n",
       "6    10   90              [75, 10, 15]          0.373687\n",
       "7    10   90          [10, 68, 12, 10]          0.391894\n",
       "8    10   90      [10, 10, 60, 10, 10]          0.385096\n",
       "9    10   90  [10, 50, 10, 10, 10, 10]          0.345982\n",
       "10   10   85                  [15, 85]          0.358555\n",
       "11   10   85              [75, 10, 15]          0.373687\n",
       "12   10   85          [10, 68, 12, 10]          0.391894\n",
       "13   10   85      [10, 10, 60, 10, 10]          0.385096\n",
       "14   10   85  [10, 50, 10, 10, 10, 10]          0.345982\n",
       "15   10   80                  [78, 22]          0.356513\n",
       "16   10   80              [75, 10, 15]          0.373687\n",
       "17   10   80          [10, 68, 12, 10]          0.391894\n",
       "18   10   80      [10, 10, 60, 10, 10]          0.385096\n",
       "19   10   80  [10, 50, 10, 10, 10, 10]          0.345982\n",
       "20   15   80                  [78, 22]          0.356513\n",
       "21   15   80              [69, 15, 16]          0.357491\n",
       "22   15   80          [55, 15, 15, 15]          0.341910\n",
       "23   15   80      [15, 15, 40, 15, 15]          0.268603\n",
       "24   15   80  [25, 15, 15, 15, 15, 15]         -0.005482\n",
       "25   15   75                  [25, 75]          0.341501\n",
       "26   15   75              [69, 15, 16]          0.357491\n",
       "27   15   75          [55, 15, 15, 15]          0.341910\n",
       "28   15   75      [15, 15, 40, 15, 15]          0.268603\n",
       "29   15   75  [25, 15, 15, 15, 15, 15]         -0.005482"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constrained_df = pd.DataFrame(constrained_stats, columns=['min','max','bases','silhouette_score'])\n",
    "constrained_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "constrained_df.to_csv(\"testing_different_constraints.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-10-f5b40a3050c0>, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-f5b40a3050c0>\"\u001b[1;36m, line \u001b[1;32m58\u001b[0m\n\u001b[1;33m    #**********************************************************************#\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('pandas_multiple.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Create dataframe for storing all cluster/variable combo averages and stdevs\n",
    "cls_averages_all = pd.DataFrame()\n",
    "\n",
    "# All maps for all cluster solutions\n",
    "all_maps = []\n",
    "\n",
    "# Column of last cluster solution\n",
    "first_var = 18\n",
    "last_var = op.shape[1]\n",
    "\n",
    "# Maximum number of clusters\n",
    "max_clusters = 6\n",
    "\n",
    "for n in range(2, max_clusters+1):\n",
    "    \n",
    "    sw = []\n",
    "    \n",
    "    # Create clustering objects\n",
    "    cls1 = KMeans(n_clusters=n, random_state=0)\n",
    "    cls2 = KMedoids(n_clusters=n, random_state=0)\n",
    "    cls3 = AgglomerativeClustering(n_clusters=n,\n",
    "                                   affinity='euclidean',\n",
    "                                   linkage='ward')\n",
    "        # Agglomerative clustering: if linkage=ward, affinity must be Euclidean\n",
    "    cls_algs = [['kMeans', cls1],\n",
    "                ['kMedclf.cluster_centers_oids', cls2],\n",
    "                ['Hierarchical', cls3]]\n",
    "    \n",
    "    # Fit and score clustering solutions for i clusters w/ each algorithm\n",
    "    for cls in cls_algs:\n",
    "        \n",
    "        # Fit the model to the factor analysis scores\n",
    "        cls[1].fit(scores)\n",
    "        \n",
    "        # List of assigned clusters\n",
    "        clusters = cls[1].fit_predict(scores)\n",
    "        \n",
    "        # Silhouette scores for each solution\n",
    "        silhouette_avg = silhouette_score(scores,clusters)\n",
    "        \n",
    "        # Store solution info\n",
    "        algorithm = cls[0]\n",
    "        n_stats = [algorithm, n, silhouette_avg, clusters]\n",
    "        sw.append(n_stats)\n",
    "\n",
    "    # Reorder cluster lists by descending silhouette scores.\n",
    "    # Clusters in first element should be assigned to training data.\n",
    "    sw = sorted(sw, key=itemgetter(2), reverse=True)\n",
    "    op[f'Optimal {sw[0][1]} cluster solution ({sw[0][0]})'] = sw[0][3] + 1\n",
    "    \n",
    "    print(sw)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #**********************************************************************#\n",
    "    # This is where the classification stuff begins\n",
    "\n",
    "for i in range(18,last_var):\n",
    "\n",
    "\n",
    "    df_cl = op.iloc[:,np.r_[2:18,i]]  # i is the current cluster solution\n",
    "    df_cl_cons = op.iloc[:,np.r_[1:18,i]]  # Same as df_cl but with constant\n",
    "\n",
    "    #**********************************************************************#\n",
    "\n",
    "    # Split data into 70% training, 30% validation\n",
    "    train, valid = train_test_split(df_cl, test_size=0.30, random_state=123)\n",
    "\n",
    "    # X is unlabeled training data, y is true training labels \n",
    "    X, y = train.iloc[:,0:-1], train.iloc[:,-1]\n",
    "\n",
    "    X_valid, y_valid = valid.iloc[:,0:-1], valid.iloc[:,-1]\n",
    "\n",
    "    #**********************************************************************#\n",
    "\n",
    "    # Get variable importances\n",
    "\n",
    "    clf1 = RandomForestClassifier(random_state=0)\n",
    "    clf2 = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "    classifiers = [['rf', clf1], ['gbt', clf2]]\n",
    "\n",
    "    for classifier in classifiers:    \n",
    "        # Fit classifier to training data\n",
    "        classifier[1].fit(X,y)    \n",
    "\n",
    "    # Create variable importance dataframe\n",
    "    num_vars = list(range(1,len(clf1.feature_importances_)+1))\n",
    "    importance = pd.DataFrame({'variable': num_vars,\n",
    "                               'rf': clf1.feature_importances_,\n",
    "                               'gbt': clf2.feature_importances_,})\n",
    "\n",
    "    # Average variable importance of rf and gbt models\n",
    "    importance['avg'] = (importance['rf']+importance['gbt'])/2\n",
    "\n",
    "    # Put avg importances on a scale from 0 to 1 to make it easier to visualize\n",
    "    importance['Relative Importance'] = np.interp(importance['avg'],\n",
    "                                                  (importance['avg'].min(),\n",
    "                                                   importance['avg'].max()),\n",
    "                                                  (0, 1))\n",
    "\n",
    "    # View top 10 variables when RF and GBT models are averaged\n",
    "    top_10_avg = importance.sort_values(by='avg', ascending=False)[['avg','Relative Importance']].head(10)\n",
    "\n",
    "    # Add variable rank column to dataframe\n",
    "    importance_rank = num_vars\n",
    "    importance = importance.sort_values(by='Relative Importance', ascending=False)\n",
    "    importance['rank'] = importance_rank\n",
    "    importance.reset_index(inplace=True)\n",
    "\n",
    "    # Save index of top 5 variables (not the variable number!)\n",
    "    top_5 = importance[importance['rank'] <= 5]['index']\n",
    "\n",
    "    #**********************************************************************#\n",
    "    # Average and Standard Deviations for each cluster/variable combination\n",
    "    # For cluster 1 of 2, calculate the average and stdev for each variable\n",
    "    # For cluster 2 of 2, calculate the average and stdev for each variable\n",
    "    # Etc.\n",
    "\n",
    "    if n == max_clusters:\n",
    "\n",
    "        cls_avg_list = []\n",
    "\n",
    "        # Take the mean of every variable for each cluster\n",
    "        for k in range(1, df_cl_cons.iloc[:,-1].max()+1):\n",
    "            cls_mean = df_cl_cons[df_cl_cons.iloc[:,-1] == k].iloc[:,0:-1].mean()\n",
    "            cls_mean = cls_mean.append(pd.Series({\"Count\":df_cl[df_cl.iloc[:,-1] == k].iloc[:,0:-1].shape[0]}))\n",
    "            cls_avg_list.append(cls_mean)\n",
    "            cls_std = df_cl_cons[df_cl_cons.iloc[:,-1] == k].iloc[:,0:-1].std()\n",
    "            cls_avg_list.append(cls_std)\n",
    "            # NaN means there is either only 1 observation in that cluster or none.\n",
    "        \n",
    "        # Convert to dataframe and transpose\n",
    "        cls_averages = pd.DataFrame(cls_avg_list)\n",
    "        cls_averages = cls_averages.T\n",
    "\n",
    "        # Create helpful column names (Cluster # of total_#)\n",
    "        col_names = []\n",
    "        \n",
    "        for col in range(1, k+1):\n",
    "            new_name1 = f\"Avg cluster {col}/{k}\"\n",
    "            col_names.append(new_name1)\n",
    "            new_name2 = f\"Std cluster {col}/{k}\"\n",
    "            col_names.append(new_name2)            \n",
    "            \n",
    "        # Rename columns\n",
    "        cls_averages.columns = col_names\n",
    "\n",
    "        cls_averages_all = pd.concat([cls_averages_all, cls_averages], axis=1)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    #**********************************************************************#\n",
    "    # Convert data to binary, train classifiers, score validation, create maps\n",
    "\n",
    "    # Convert X, X_valid, and df_cl predictors to all 1 and -1\n",
    "    X = (X.mask(df > 0, other=1, inplace=False)\n",
    "         .mask(df <= 0, other=-1, inplace=False))\n",
    "    X_valid = (X_valid.mask(df > 0, other=1, inplace=False)\n",
    "               .mask(df <= 0, other=-1, inplace=False))\n",
    "    all_data_masked = (df_cl.iloc[:,0:-1].mask(df > 0, other=1, inplace=False)\n",
    "                       .mask(df <= 0, other=-1, inplace=False))\n",
    "\n",
    "    map_collection = []\n",
    "\n",
    "    # Retrain on the 2-5 most important variables\n",
    "    for j in range(2,6):\n",
    "\n",
    "        clf_scores = []\n",
    "\n",
    "        clf1 = RandomForestClassifier(random_state=0)\n",
    "        clf2 = GradientBoostingClassifier(random_state=0)\n",
    "        clf3 = SVC(random_state=0)\n",
    "        clf4 = KNeighborsClassifier()\n",
    "\n",
    "        classifiers = [['rf', clf1], ['gbt', clf2], ['svc', clf3], ['knn', clf4]]\n",
    "\n",
    "        # Fit each classifier to the current variable/cluster combination\n",
    "        for classifier in classifiers:\n",
    "\n",
    "            # Fit classifier to training data\n",
    "            classifier[1].fit(X.iloc[:,np.r_[top_5[0:j]]],y)\n",
    "\n",
    "            # Store classifier-specific results [algorithm object, classifier name, scores]\n",
    "            results = [classifier[1],\n",
    "                       classifier[0],\n",
    "                       classifier[1].score(X_valid.iloc[:,np.r_[top_5[0:j]]],y_valid)]\n",
    "\n",
    "            # Overall classifier results\n",
    "            clf_scores.append(results)\n",
    "\n",
    "        # Sort classifier accuracy in descending order\n",
    "        clf_scores = sorted(clf_scores, key=itemgetter(2), reverse=True)\n",
    "        # clf_scores[0][0] is the best model\n",
    "\n",
    "        # Fit the best model on all data\n",
    "        best_model = clf_scores[0][0].fit(all_data_masked.iloc[:,np.r_[top_5[0:j]]], df_cl.iloc[:,-1])\n",
    "\n",
    "        #******************************************************************#\n",
    "        # Create mappings\n",
    "\n",
    "        # Creates grid of dimension j\n",
    "        grid = pd.DataFrame(list(itertools.product([-1,1], repeat=j)))\n",
    "\n",
    "        grid.columns = all_data_masked.iloc[:,np.r_[top_5[0:j]]].columns\n",
    "\n",
    "        # This is the best model predicting the grid\n",
    "        preds = best_model.predict(grid)            \n",
    "\n",
    "        # Add to grid dataframe\n",
    "        grid['Predicted Cluster'] = preds\n",
    "\n",
    "        # Change grid to mapping to fit into the rest of the code\n",
    "        mapping = grid\n",
    "\n",
    "        # Save current mapping to map collection for this cluster solution\n",
    "        map_collection.append(mapping)\n",
    "\n",
    "        # Write each dataframe to a different worksheet.\n",
    "        mapping.to_excel(writer, index=False, sheet_name=f\"{df_cl.columns[-1][8:17]}s, {j} vars, {round(clf_scores[0][2]*100)}% Acc.\")\n",
    "\n",
    "    all_maps.append(map_collection)\n",
    "\n",
    "op.to_excel(writer, index=False, sheet_name=\"All Regressions, Clusters\")\n",
    "\n",
    "\n",
    "# Add averages for all observations to cls_averages_all before exporting\n",
    "all_obs = []\n",
    "\n",
    "# Variable means for all observations\n",
    "all_obs_mean = list(op.filter(regex='^[a-zA-Z][0-9]').mean().values)\n",
    "all_obs_mean.insert(0,op['Const'].mean())\n",
    "all_obs.append(all_obs_mean)\n",
    "\n",
    "cls_averages_all['new_col'] = pd.Series(list(op.filter(regex='^[a-zA-Z][0-9]').mean().values))\n",
    "\n",
    "# Variable standard deviations for all observations\n",
    "all_obs_std = list(op.filter(regex='^[a-zA-Z][0-9]').std().values)\n",
    "all_obs_std.insert(0,op['Const'].std())\n",
    "all_obs.append(all_obs_std)\n",
    "\n",
    "\n",
    "# Save as dataframe and append to all cls_averages_all dataframe\n",
    "all_obs_cols = list(op.filter(regex='^[a-zA-Z][0-9]').columns)\n",
    "all_obs_cols.insert(0, \"Const\")\n",
    "all_obs_df = pd.DataFrame(all_obs, columns=all_obs_cols)\n",
    "all_obs_df = all_obs_df.T\n",
    "all_obs_cols = ['All obs avg', 'All obs stdev']\n",
    "all_obs_df.columns = all_obs_cols\n",
    "cls_averages_all = pd.concat([cls_averages_all, all_obs_df], axis=1)\n",
    "\n",
    "\n",
    "cls_averages_all.to_excel(writer, sheet_name=\"Cluster Avgs and StDevs\")\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of script. Code below is for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for making the letter-based regression dynamic\n",
    "\n",
    "# Create a list of unique first letters of variables\n",
    "X = df.drop(['UID','Rating','target'], axis=1)\n",
    "y = df[\"target\"]\n",
    "\n",
    "var_letters = []\n",
    "\n",
    "for i in X.columns:\n",
    "    var_letters.append(i[0:1])  # Append first character\n",
    "\n",
    "var_letters = list(np.unique(var_letters))  # List of unique variable letters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Group Regressions & PCA Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "### Hidden: Code for the regressions on variable groups A, B, C, D\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# ### Linear Regression with A Variables\n",
    "\n",
    "# # Unique IDs\n",
    "# ids = df.UID.unique()\n",
    "\n",
    "# # Run linear regressions for each UID\n",
    "# op = pd.DataFrame\n",
    "# intercept = []\n",
    "# coefficients=[]\n",
    "# UID = []\n",
    "# for i in ids:\n",
    "#     df_i = df[df.UID == i]              # Create dataframe for current user id\n",
    "#     X = df_i.drop(['UID','B1', 'B2', 'B3', 'B4','C1', 'C2', 'C3', 'C4','D1', 'D2', 'D3', 'D4','Rating','target'], axis=1)  # df input variables only\n",
    "#     y = df_i['target']                  # Series of target variable\n",
    "#     reg = LinearRegression().fit(X, y)  # Fit linear regression\n",
    "#     reg.score(X, y)                     # Score regression model\n",
    "#     unique_id=df_i['UID'].unique()      # Saves current user id\n",
    "#     const = reg.intercept_              # Save intercept of the regression model\n",
    "#     coef = reg.coef_                    # Coefficients of regression model\n",
    "#     UID.append(unique_id)               # Append current user id\n",
    "#     intercept.append(const)             # Append current intercept\n",
    "#     coefficients.append(coef)           # Append current regression coefficients\n",
    "\n",
    "# # Convert newly created lists into dataframes\n",
    "# intercep_new = pd.DataFrame(intercept)\n",
    "# coefficients_new = pd.DataFrame(coefficients)\n",
    "# UID_new = pd.DataFrame(UID)\n",
    "\n",
    "# # Get columns names\n",
    "# colNames = df.drop(['B1', 'B2', 'B3', 'B4','C1', 'C2', 'C3', 'C4','D1', 'D2', 'D3', 'D4','Rating', 'target'], axis=1).columns\n",
    "# colNames = colNames.insert(1, 'Const')\n",
    "# colNames\n",
    "\n",
    "# # Concatenate the new dataframes and add column names\n",
    "# op = pd.concat([UID_new,intercep_new, coefficients_new], axis=1)\n",
    "# op.columns = colNames\n",
    "# op_A = op.iloc[:,2:6]\n",
    "\n",
    "\n",
    "# #******************************************************************************#\n",
    "# ### Linear Regression with B Variables\n",
    "\n",
    "# # Unique IDs\n",
    "# ids = df.UID.unique()\n",
    "\n",
    "# # Run linear regressions for each UID\n",
    "# op = pd.DataFrame\n",
    "# intercept = []\n",
    "# coefficients=[]\n",
    "# UID = []\n",
    "# for i in ids:\n",
    "#     df_i = df[df.UID == i]              # Create dataframe for current user id\n",
    "#     X = df_i.drop(['UID','A1', 'A2', 'A3', 'A4','C1', 'C2', 'C3', 'C4','D1', 'D2', 'D3', 'D4','Rating','target'], axis=1)  # df input variables only\n",
    "#     y = df_i['target']                  # Series of target variable\n",
    "#     reg = LinearRegression().fit(X, y)  # Fit linear regression\n",
    "#     reg.score(X, y)                     # Score regression model\n",
    "#     unique_id=df_i['UID'].unique()      # Saves current user id\n",
    "#     const = reg.intercept_              # Save intercept of the regression model\n",
    "#     coef = reg.coef_                    # Coefficients of regression model\n",
    "#     UID.append(unique_id)               # Append current user id\n",
    "#     intercept.append(const)             # Append current intercept\n",
    "#     coefficients.append(coef)           # Append current regression coefficients\n",
    "\n",
    "# # Convert newly created lists into dataframes\n",
    "# intercep_new = pd.DataFrame(intercept)\n",
    "# coefficients_new = pd.DataFrame(coefficients)\n",
    "# UID_new = pd.DataFrame(UID)\n",
    "\n",
    "# # Get columns names\n",
    "# colNames = df.drop(['A1', 'A2', 'A3', 'A4','C1', 'C2', 'C3', 'C4','D1', 'D2', 'D3', 'D4','Rating', 'target'], axis=1).columns\n",
    "# colNames = colNames.insert(1, 'Const')\n",
    "# colNames\n",
    "\n",
    "# # Concatenate the new dataframes and add column names\n",
    "# op = pd.concat([UID_new,intercep_new, coefficients_new], axis=1)\n",
    "# op.columns = colNames\n",
    "# op_B = op.iloc[:,2:6]\n",
    "\n",
    "\n",
    "# #******************************************************************************#\n",
    "# ### Linear Regression with C Variables\n",
    "\n",
    "# # Unique IDs\n",
    "# ids = df.UID.unique()\n",
    "\n",
    "# # Run linear regressions for each UID\n",
    "# op = pd.DataFrame\n",
    "# intercept = []\n",
    "# coefficients=[]\n",
    "# UID = []\n",
    "# for i in ids:\n",
    "#     df_i = df[df.UID == i]              # Create dataframe for current user id\n",
    "#     X = df_i.drop(['UID', 'A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'B3', 'B4', 'D1', 'D2', 'D3', 'D4', 'Rating', 'target'], axis=1)  # df input variables only\n",
    "#     y = df_i['target']                  # Series of target variable\n",
    "#     reg = LinearRegression().fit(X, y)  # Fit linear regression\n",
    "#     reg.score(X, y)                     # Score regression model\n",
    "#     unique_id=df_i['UID'].unique()      # Saves current user id\n",
    "#     const = reg.intercept_              # Save intercept of the regression model\n",
    "#     coef = reg.coef_                    # Coefficients of regression model\n",
    "#     UID.append(unique_id)               # Append current user id\n",
    "#     intercept.append(const)             # Append current intercept\n",
    "#     coefficients.append(coef)           # Append current regression coefficients\n",
    "\n",
    "# # Convert newly created lists into dataframes\n",
    "# intercep_new = pd.DataFrame(intercept)\n",
    "# coefficients_new = pd.DataFrame(coefficients)\n",
    "# UID_new = pd.DataFrame(UID)\n",
    "\n",
    "# # Get columns names\n",
    "# colNames = df.drop(['A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'B3', 'B4', 'D1', 'D2', 'D3', 'D4', 'Rating', 'target'], axis=1).columns\n",
    "# colNames = colNames.insert(1, 'Const')\n",
    "# colNames\n",
    "\n",
    "# # Concatenate the new dataframes and add column names\n",
    "# op = pd.concat([UID_new,intercep_new, coefficients_new], axis=1)\n",
    "# op.columns = colNames\n",
    "# op_C = op.iloc[:,2:6]\n",
    "\n",
    "\n",
    "\n",
    "# #******************************************************************************#\n",
    "# ### Linear Regression with D Variables\n",
    "\n",
    "# # Unique IDs\n",
    "# ids = df.UID.unique()\n",
    "\n",
    "# # Run linear regressions for each UID\n",
    "# op = pd.DataFrame\n",
    "# intercept = []\n",
    "# coefficients=[]\n",
    "# UID = []\n",
    "# for i in ids:\n",
    "#     df_i = df[df.UID == i]              # Create dataframe for current user id\n",
    "#     X = df_i.drop(['UID', 'A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'B3', 'B4', 'C1', 'C2', 'C3', 'C4', 'Rating', 'target'], axis=1)  # df input variables only\n",
    "#     y = df_i['target']                  # Series of target variable\n",
    "#     reg = LinearRegression().fit(X, y)  # Fit linear regression\n",
    "#     reg.score(X, y)                     # Score regression model\n",
    "#     unique_id=df_i['UID'].unique()      # Saves current user id\n",
    "#     const = reg.intercept_              # Save intercept of the regression model\n",
    "#     coef = reg.coef_                    # Coefficients of regression model\n",
    "#     UID.append(unique_id)               # Append current user id\n",
    "#     intercept.append(const)             # Append current intercept\n",
    "#     coefficients.append(coef)           # Append current regression coefficients\n",
    "\n",
    "# # Convert newly created lists into dataframes\n",
    "# intercep_new = pd.DataFrame(intercept)\n",
    "# coefficients_new = pd.DataFrame(coefficients)\n",
    "# UID_new = pd.DataFrame(UID)\n",
    "\n",
    "# # Get columns names\n",
    "# colNames = df.drop(['A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'B3', 'B4', 'C1', 'C2', 'C3', 'C4', 'Rating', 'target'], axis=1).columns\n",
    "# colNames = colNames.insert(1, 'Const')\n",
    "# colNames\n",
    "\n",
    "# # Concatenate the new dataframes and add column names\n",
    "# op = pd.concat([UID_new,intercep_new, coefficients_new], axis=1)\n",
    "# op.columns = colNames\n",
    "# op_D = op.iloc[:,2:6]\n",
    "\n",
    "\n",
    "# #******************************************************************************#\n",
    "# ### Replace 1's w/ Regression Coefficients in Original Data\n",
    "\n",
    "# # Concatenate regression dataframes\n",
    "# all_cfs = pd.concat([op_A, op_B, op_C, op_D], axis=1)\n",
    "\n",
    "# # Replace 1's w/ regression coefficients by column\n",
    "# cfs_cols = all_cfs.columns\n",
    "\n",
    "# for col in cfs_cols:\n",
    "#     for i in range(1,len(all_cfs)+1):\n",
    "#         df.loc[df['UID'] == i,[col]] = df.loc[df['UID'] == i,[col]].replace(1,all_cfs.loc[i-1,col])\n",
    "\n",
    "# df.head()\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "## PCA on Regression Coefficients\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# #******************************************************************************#\n",
    "# # Comparing covariance and correlation PCA.  We should do correlation PCA.\n",
    "\n",
    "# # Create PCA dataframe\n",
    "# df_fct = op.drop(['UID','Const'], axis=1)\n",
    "\n",
    "# # Standardize df_fct\n",
    "# # df_pca = stats.zscore(df_fct)  # Didn't need to worry about standardizing.\n",
    "# # Could build in a correlation/covariance PCA thing using ranges of the variables\n",
    "\n",
    "# # Create PCA object\n",
    "# pca = PCA(random_state=123)\n",
    "\n",
    "# # Get principal components\n",
    "# pca.fit(df_fct)\n",
    "\n",
    "# # Get scores\n",
    "# pca.transform(df_fct)\n",
    "\n",
    "# # Components needed\n",
    "# pcs_needed = len(np.where(pca.explained_variance_ >= 1)[0])\n",
    "\n",
    "# # Save scores for PCs w/ eignvalues >=1 as dataframe for clustering\n",
    "# scores = pd.DataFrame(pca.transform(df_fct))\n",
    "# scores = scores.iloc[:,0:pcs_needed]\n",
    "\n",
    "# # How many observations\n",
    "# n_samples = pca.components_.shape[0]\n",
    "\n",
    "# # Transpose the principal components\n",
    "# murph = pca.components_.T\n",
    "# # Center the data\n",
    "# murph -= np.mean(murph, axis=0)\n",
    "# # Compute the covariance matrix\n",
    "# cov_matrix = np.dot(murph.T, murph) / n_samples\n",
    "\n",
    "# for eigenvector in pca.components_:\n",
    "#     print(np.dot(eigenvector.T, np.dot(cov_matrix, eigenvector)))\n",
    "\n",
    "# n_samples = df_fct.shape[0]\n",
    "\n",
    "# pca = PCA(random_state=123)\n",
    "# X_transformed = pca.fit_transform(df_fct)\n",
    "\n",
    "# # This is the explained variance.  They are big numbers because this is Cov. PCA\n",
    "# X_centered = df_fct - np.mean(df_fct, axis=0)\n",
    "# cov_matrix = np.dot(X_centered.T, X_centered) / n_samples\n",
    "# eigenvalues = pca.explained_variance_\n",
    "# for eigenvalue, eigenvector in zip(eigenvalues, pca.components_):\n",
    "#     print(eigenvalue)\n",
    "\n",
    "# # This is the same thing as the code above but built into sklearn\n",
    "# pca.explained_variance_\n",
    "# # These are the eigenvalues of the covariance matrix\n",
    "# # They don't have values near 1-ish, so I need to understand why.\n",
    "\n",
    "\n",
    "# #******************************************************************************#\n",
    "# # Biplots for comparing correlation and covariance PCA for this data\n",
    "\n",
    "# # !pip install pca\n",
    "# from pca import pca\n",
    "\n",
    "# # Correlation PCA & biplot\n",
    "# model = pca(n_components=4)\n",
    "# results = model.fit_transform(df_pca)\n",
    "# fig, ax = model.biplot(n_feat=16, cmap=None, label=False, legend=False)\n",
    "\n",
    "# # Covariance PCA & biplot\n",
    "# model2 = pca(n_components=4)\n",
    "# results2 = model2.fit_transform(df_fct)\n",
    "# fig, ax = model2.biplot(n_feat=16, cmap=None, label=False, legend=False)\n",
    "\n",
    "# # Create PCA dataframe\n",
    "# df_fct = op.drop(['UID','Const'], axis=1)\n",
    "\n",
    "# # Standardize df_fct - Need to standardize to use eigenvalues >= 1\n",
    "# df_fct = stats.zscore(df_fct)\n",
    "\n",
    "# # Create PCA object\n",
    "# pca = PCA(random_state=123)\n",
    "\n",
    "# # Get principal components & scores\n",
    "# pca.fit_transform(df_fct)\n",
    "\n",
    "# # Components needed\n",
    "# pcs_needed = len(np.where(pca.explained_variance_ >= 1)[0])\n",
    "\n",
    "# # Save scores for PCs w/ eignvalues >=1 as dataframe for clustering\n",
    "# scores = pd.DataFrame(pca.transform(df_fct))\n",
    "# scores = scores.iloc[:,0:pcs_needed]\n",
    "# expl_var = round(pca.explained_variance_ratio_.cumsum()[pcs_needed-1]*100, 2)\n",
    "\n",
    "# print(\"Principal Components Used: \", pcs_needed, sep='')\n",
    "# print(\"Variance Explained: \", expl_var, \"%\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with SMOTE for synthetic data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# EXPERIMENTING WITH SMOTE\n",
    "################################################################################\n",
    "\n",
    "# Maximum number of clusters\n",
    "max_clusters = 6\n",
    "\n",
    "for n in range(2, max_clusters+1):\n",
    "    \n",
    "    sw = []\n",
    "    \n",
    "    # Create clustering objects\n",
    "    cls1 = KMeans(n_clusters=n, random_state=0)\n",
    "    cls2 = KMedoids(n_clusters=n, random_state=0)\n",
    "    cls3 = AgglomerativeClustering(n_clusters=n,\n",
    "                                   affinity='euclidean',\n",
    "                                   linkage='ward')\n",
    "        # Agglomerative clustering: if linkage=ward, affinity must be Euclidean\n",
    "    cls_algs = [['kMeans', cls1],\n",
    "                ['kMedoids', cls2],\n",
    "                ['Hierarchical', cls3]]\n",
    "    \n",
    "    # Fit and score clustering solutions for i clusters w/ each algorithm\n",
    "    for cls in cls_algs:\n",
    "        \n",
    "        # Fit the model to the factor analysis scores\n",
    "        cls[1].fit(scores)\n",
    "        \n",
    "        # List of assigned clusters\n",
    "        clusters = cls[1].fit_predict(scores)\n",
    "        \n",
    "        # Silhouette scores for each solution\n",
    "        silhouette_avg = silhouette_score(scores,clusters)\n",
    "        \n",
    "        # Store solution info\n",
    "        algorithm = cls[0]\n",
    "        n_stats = [algorithm, n, silhouette_avg, clusters]\n",
    "        sw.append(n_stats)\n",
    "\n",
    "    # Reorder cluster lists by descending silhouette scores.\n",
    "    # Clusters in first element should be assigned to training data.\n",
    "    sw = sorted(sw, key=itemgetter(2), reverse=True)\n",
    "    op[f'Optimal {sw[0][1]} cluster solution ({sw[0][0]})'] = sw[0][3] + 1\n",
    "\n",
    "import imblearn\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = op.filter(regex='^[a-zA-Z][0-9]')\n",
    "y = op.iloc[:,18]\n",
    "\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "op.head()\n",
    "\n",
    "X['Optimal 2 cluster solution (Hierarchical)'] = y\n",
    "X['UID'] = op.UID\n",
    "X['Const'] = op.Const\n",
    "# X = X.iloc[:, np.r_[-2,-1,0:-2]]\n",
    "\n",
    "cols = X.columns.tolist()\n",
    "cols = cols[-2:] + cols[:-2]\n",
    "\n",
    "X = X[cols]\n",
    "\n",
    "op = X\n",
    "\n",
    "op.shape\n",
    "\n",
    "op.head()\n",
    "\n",
    "for i in range(18,19):\n",
    "\n",
    "\n",
    "    df_cl = op.iloc[:,np.r_[2:18,i]]  # i is the current cluster solution\n",
    "\n",
    "    #**********************************************************************#\n",
    "\n",
    "    # Split data into 70% training, 30% validation\n",
    "    train, valid = train_test_split(df_cl, test_size=0.30, random_state=123)\n",
    "\n",
    "    # X is unlabeled training data, y is true training labels \n",
    "    X, y = train.iloc[:,0:-1], train.iloc[:,-1]\n",
    "\n",
    "    X_valid, y_valid = valid.iloc[:,0:-1], valid.iloc[:,-1]\n",
    "\n",
    "    #**********************************************************************#\n",
    "\n",
    "    # Get variable importances\n",
    "\n",
    "    clf1 = RandomForestClassifier(random_state=0)\n",
    "    clf2 = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "    classifiers = [['rf', clf1], ['gbt', clf2]]\n",
    "\n",
    "    for classifier in classifiers:    \n",
    "        # Fit classifier to training data\n",
    "        classifier[1].fit(X,y)    \n",
    "\n",
    "    # Create variable importance dataframe\n",
    "    num_vars = list(range(1,len(clf1.feature_importances_)+1))\n",
    "    importance = pd.DataFrame({'variable': num_vars,\n",
    "                               'rf': clf1.feature_importances_,\n",
    "                               'gbt': clf2.feature_importances_,})\n",
    "\n",
    "    # Average variable importance of rf and gbt models\n",
    "    importance['avg'] = (importance['rf']+importance['gbt'])/2\n",
    "\n",
    "    # Put avg importances on a scale from 0 to 1 to make it easier to visualize\n",
    "    importance['Relative Importance'] = np.interp(importance['avg'],\n",
    "                                                  (importance['avg'].min(),\n",
    "                                                   importance['avg'].max()),\n",
    "                                                  (0, 1))\n",
    "\n",
    "    # View top 10 variables when RF and GBT models are averaged\n",
    "    top_10_avg = importance.sort_values(by='avg', ascending=False)[['avg','Relative Importance']].head(10)\n",
    "\n",
    "    # Add variable rank column to dataframe\n",
    "    importance_rank = num_vars\n",
    "    importance = importance.sort_values(by='Relative Importance', ascending=False)\n",
    "    importance['rank'] = importance_rank\n",
    "    importance.reset_index(inplace=True)\n",
    "\n",
    "    # Save index of top 5 variables (not the variable number!)\n",
    "    top_5 = importance[importance['rank'] <= 5]['index']\n",
    "\n",
    "    #**********************************************************************#\n",
    "    # Average and Standard Deviations for each cluster/variable combination\n",
    "    # For cluster 1 of 2, calculate the average and stdev for each variable\n",
    "    # For cluster 2 of 2, calculate the average and stdev for each variable\n",
    "    # Etc.\n",
    "\n",
    "#     if n == max_clusters:\n",
    "\n",
    "#         cls_avg_list = []\n",
    "\n",
    "#         # Take the mean of every variable for each cluster\n",
    "#         for k in range(1, df_cl_cons.iloc[:,-1].max()+1):\n",
    "#             cls_mean = df_cl_cons[df_cl_cons.iloc[:,-1] == k].iloc[:,0:-1].mean()\n",
    "#             cls_mean = cls_mean.append(pd.Series({\"Count\":df_cl[df_cl.iloc[:,-1] == k].iloc[:,0:-1].shape[0]}))\n",
    "#             cls_avg_list.append(cls_mean)\n",
    "#             cls_std = df_cl_cons[df_cl_cons.iloc[:,-1] == k].iloc[:,0:-1].std()\n",
    "#             cls_avg_list.append(cls_std)\n",
    "#             # NaN means there is either only 1 observation in that cluster or none.\n",
    "        \n",
    "#         # Convert to dataframe and transpose\n",
    "#         cls_averages = pd.DataFrame(cls_avg_list)\n",
    "#         cls_averages = cls_averages.T\n",
    "\n",
    "#         # Create helpful column names (Cluster # of total_#)\n",
    "#         col_names = []\n",
    "        \n",
    "#         for col in range(1, k+1):\n",
    "#             new_name1 = f\"Avg cluster {col}/{k}\"\n",
    "#             col_names.append(new_name1)\n",
    "#             new_name2 = f\"Std cluster {col}/{k}\"\n",
    "#             col_names.append(new_name2)            \n",
    "            \n",
    "#         # Rename columns\n",
    "#         cls_averages.columns = col_names\n",
    "\n",
    "#         cls_averages_all = pd.concat([cls_averages_all, cls_averages], axis=1)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    #**********************************************************************#\n",
    "    # Convert data to binary, train classifiers, score validation, create maps\n",
    "\n",
    "    # Convert X, X_valid, and df_cl predictors to all 1 and -1\n",
    "#     X = (X.mask(df > 0, other=1, inplace=False)\n",
    "#          .mask(df <= 0, other=-1, inplace=False))\n",
    "#     X_valid = (X_valid.mask(df > 0, other=1, inplace=False)\n",
    "#                .mask(df <= 0, other=-1, inplace=False))\n",
    "    all_data_masked = (df_cl.iloc[:,0:-1]) #.mask(df > 0, other=1, inplace=False)\n",
    "#                        .mask(df <= 0, other=-1, inplace=False))\n",
    "\n",
    "    map_collection = []\n",
    "\n",
    "    # Retrain on the 2-5 most important variables\n",
    "    for j in range(2,6):\n",
    "\n",
    "        clf_scores = []\n",
    "\n",
    "        clf1 = RandomForestClassifier(random_state=0)\n",
    "        clf2 = GradientBoostingClassifier(random_state=0)\n",
    "        clf3 = SVC(random_state=0)\n",
    "        clf4 = KNeighborsClassifier()\n",
    "\n",
    "        classifiers = [['rf', clf1], ['gbt', clf2], ['svc', clf3], ['knn', clf4]]\n",
    "\n",
    "        # Fit each classifier to the current variable/cluster combination\n",
    "        for classifier in classifiers:\n",
    "\n",
    "            # Fit classifier to training data\n",
    "            classifier[1].fit(X.iloc[:,np.r_[top_5[0:j]]],y)\n",
    "            print(classifier[1].predict(X.iloc[:,np.r_[top_5[0:j]]]))\n",
    "\n",
    "            # Store classifier-specific results [algorithm object, classifier name, scores]\n",
    "            results = [classifier[1],\n",
    "                       classifier[0],\n",
    "                       classifier[1].score(X_valid.iloc[:,np.r_[top_5[0:j]]],y_valid)]\n",
    "\n",
    "            # Overall classifier results\n",
    "            clf_scores.append(results)\n",
    "\n",
    "        # Sort classifier accuracy in descending order\n",
    "        clf_scores = sorted(clf_scores, key=itemgetter(2), reverse=True)\n",
    "        # clf_scores[0][0] is the best model\n",
    "\n",
    "        # Fit the best model on all data\n",
    "        best_model = clf_scores[0][0].fit(all_data_masked.iloc[:,np.r_[top_5[0:j]]], df_cl.iloc[:,-1])\n",
    "        \n",
    "        print(j)\n",
    "        print(clf_scores)\n",
    "\n",
    "#         #******************************************************************#\n",
    "#         # Create mappings\n",
    "\n",
    "#         # Creates grid of dimension j\n",
    "#         grid = pd.DataFrame(list(itertools.product([-1,1], repeat=j)))\n",
    "\n",
    "#         grid.columns = all_data_masked.iloc[:,np.r_[top_5[0:j]]].columns\n",
    "\n",
    "#         # This is the best model predicting the grid\n",
    "#         preds = best_model.predict(grid)            \n",
    "\n",
    "#         # Add to grid dataframe\n",
    "#         grid['Predicted Cluster'] = preds\n",
    "\n",
    "#         # Change grid to mapping to fit into the rest of the code\n",
    "#         mapping = grid\n",
    "\n",
    "#         # Save current mapping to map collection for this cluster solution\n",
    "#         map_collection.append(mapping)\n",
    "\n",
    "#         # Write each dataframe to a different worksheet.\n",
    "#         mapping.to_excel(writer, index=False, sheet_name=f\"{df_cl.columns[-1][8:17]}s, {j} vars, {round(clf_scores[0][2]*100)}% Acc.\")\n",
    "\n",
    "#     all_maps.append(map_collection)\n",
    "\n",
    "# op.to_excel(writer, index=False, sheet_name=\"All Regressions, Clusters\")\n",
    "\n",
    "\n",
    "# # Add averages for all observations to cls_averages_all before exporting\n",
    "# all_obs = []\n",
    "\n",
    "# # Variable means for all observations\n",
    "# all_obs_mean = list(op.filter(regex='^[a-zA-Z][0-9]').mean().values)\n",
    "# all_obs_mean.insert(0,op['Const'].mean())\n",
    "# all_obs.append(all_obs_mean)\n",
    "\n",
    "# cls_averages_all['new_col'] = pd.Series(list(op.filter(regex='^[a-zA-Z][0-9]').mean()).values\n",
    "\n",
    "# # Variable standard deviations for all observations\n",
    "# all_obs_std = list(op.filter(regex='^[a-zA-Z][0-9]').std().values)\n",
    "# all_obs_std.insert(0,op['Const'].std())\n",
    "# all_obs.append(all_obs_std)\n",
    "\n",
    "\n",
    "# # Save as dataframe and append to all cls_averages_all dataframe\n",
    "# all_obs_cols = list(op.filter(regex='^[a-zA-Z][0-9]').columns)\n",
    "# all_obs_cols.insert(0, \"Const\")\n",
    "# all_obs_df = pd.DataFrame(all_obs, columns=all_obs_cols)\n",
    "# all_obs_df = all_obs_df.T\n",
    "# all_obs_cols = ['All obs avg', 'All obs stdev']\n",
    "# all_obs_df.columns = all_obs_cols\n",
    "# cls_averages_all = pd.concat([cls_averages_all, all_obs_df], axis=1)\n",
    "\n",
    "\n",
    "# cls_averages_all.to_excel(writer, sheet_name=\"Cluster Avgs and StDevs\")\n",
    "\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Scenario Analysis</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>Rating</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID  A1  A2  A3  A4  B1  B2  B3  B4  C1  C2  C3  C4  D1  D2  D3  D4  \\\n",
       "0    1   1   0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   \n",
       "1    1   1   0   0   0   0   0   1   0   0   0   0   0   1   0   0   0   \n",
       "2    1   0   0   0   1   0   0   0   1   1   0   0   0   0   0   0   0   \n",
       "3    1   0   1   0   0   0   0   0   0   0   0   0   1   0   1   0   0   \n",
       "4    1   0   1   0   0   0   0   1   0   0   1   0   0   0   0   0   1   \n",
       "\n",
       "   Rating      target  \n",
       "0     100  100.000507  \n",
       "1     100  100.000668  \n",
       "2       0    0.000386  \n",
       "3       0    0.000067  \n",
       "4       0    0.000382  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where(condition, value if condition is true, value if condition is false)\n",
    "\n",
    "# create a list of the column categories\n",
    "\n",
    "cat_A = ['A1', 'A2', 'A3', 'A4']\n",
    "cat_B = ['B1', 'B2', 'B3', 'B4']\n",
    "cat_C = ['C1', 'C2', 'C3', 'C4']\n",
    "cat_D = ['D1', 'D2', 'D3', 'D4']\n",
    "\n",
    "# create a list of our conditions\n",
    "cat_A_conditions = [\n",
    "    (df['A1'] == 1),\n",
    "    (df['A2'] == 1),\n",
    "    (df['A3'] == 1),\n",
    "    (df['A4'] == 1),\n",
    "    (df['A1']==0) & (df['A2']==0) & (df['A3']==0) & (df['A4']==0),\n",
    "    ]\n",
    "\n",
    "cat_B_conditions = [    \n",
    "    (df['B1'] == 1),\n",
    "    (df['B2'] == 1),\n",
    "    (df['B3'] == 1),\n",
    "    (df['B4'] == 1),\n",
    "    (df['B1']==0) & (df['B2']==0) & (df['B3']==0) & (df['B4']==0),\n",
    "    ]    \n",
    "\n",
    "cat_C_conditions = [    \n",
    "    (df['C1'] == 1),\n",
    "    (df['C2'] == 1),\n",
    "    (df['C3'] == 1),\n",
    "    (df['C4'] == 1),\n",
    "    (df['C1']==0) & (df['C2']==0) & (df['C3']==0) & (df['C4']==0),\n",
    "    ]\n",
    "\n",
    "cat_D_conditions = [    \n",
    "    (df['D1'] == 1),\n",
    "    (df['D2'] == 1),\n",
    "    (df['D3'] == 1),\n",
    "    (df['D4'] == 1),  \n",
    "    (df['D1']==0) & (df['D2']==0) & (df['D3']==0) & (df['D4']==0),\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "cat_A_values = ['A1', 'A2', 'A3', 'A4', 'A0']\n",
    "cat_B_values = ['B1', 'B2', 'B3', 'B4','B0']\n",
    "cat_C_values = ['C1', 'C2', 'C3', 'C4','C0']\n",
    "cat_D_values = ['D1', 'D2', 'D3', 'D4','D0']\n",
    "    \n",
    "df['cat_A_scenario'] = np.select(cat_A_conditions, cat_A_values)\n",
    "df['cat_B_scenario'] = np.select(cat_B_conditions, cat_B_values)\n",
    "df['cat_C_scenario'] = np.select(cat_C_conditions, cat_C_values)\n",
    "df['cat_D_scenario'] = np.select(cat_D_conditions, cat_D_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Example of Scenario where Variable A1 has a 1 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_A = df['cat_A_scenario'].unique()\n",
    "# list of independant variables for regression\n",
    "fields = df.columns[1:17]\n",
    "dep_var = df['noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>C1</th>\n",
       "      <th>...</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>Rating</th>\n",
       "      <th>noise</th>\n",
       "      <th>cat_A_scenario</th>\n",
       "      <th>cat_B_scenario</th>\n",
       "      <th>cat_C_scenario</th>\n",
       "      <th>cat_D_scenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000893</td>\n",
       "      <td>A1</td>\n",
       "      <td>B4</td>\n",
       "      <td>C1</td>\n",
       "      <td>D0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000783</td>\n",
       "      <td>A1</td>\n",
       "      <td>B3</td>\n",
       "      <td>C0</td>\n",
       "      <td>D1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000825</td>\n",
       "      <td>A1</td>\n",
       "      <td>B0</td>\n",
       "      <td>C1</td>\n",
       "      <td>D4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>A1</td>\n",
       "      <td>B4</td>\n",
       "      <td>C4</td>\n",
       "      <td>D3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>100.000252</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>C3</td>\n",
       "      <td>D4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000552</td>\n",
       "      <td>A1</td>\n",
       "      <td>B4</td>\n",
       "      <td>C0</td>\n",
       "      <td>D4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000693</td>\n",
       "      <td>A1</td>\n",
       "      <td>B3</td>\n",
       "      <td>C2</td>\n",
       "      <td>D3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000722</td>\n",
       "      <td>A1</td>\n",
       "      <td>B4</td>\n",
       "      <td>C4</td>\n",
       "      <td>D2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000264</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>C4</td>\n",
       "      <td>D0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000156</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>C4</td>\n",
       "      <td>D0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>513 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      UID  A1  A2  A3  A4  B1  B2  B3  B4  C1  ...  D1  D2  D3  D4  Rating  \\\n",
       "0       1   1   0   0   0   0   0   0   1   1  ...   0   0   0   0     100   \n",
       "1       1   1   0   0   0   0   0   1   0   0  ...   1   0   0   0     100   \n",
       "13      1   1   0   0   0   0   0   0   0   1  ...   0   0   0   1     100   \n",
       "16      1   1   0   0   0   0   0   0   1   0  ...   0   0   1   0       0   \n",
       "17      1   1   0   0   0   1   0   0   0   0  ...   0   0   0   1     100   \n",
       "...   ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..     ...   \n",
       "2383  100   1   0   0   0   0   0   0   1   0  ...   0   0   0   1       5   \n",
       "2385  100   1   0   0   0   0   0   1   0   0  ...   0   0   1   0       5   \n",
       "2393  100   1   0   0   0   0   0   0   1   0  ...   0   1   0   0       5   \n",
       "2398  100   1   0   0   0   1   0   0   0   0  ...   0   0   0   0       5   \n",
       "2399  100   1   0   0   0   1   0   0   0   0  ...   0   0   0   0       5   \n",
       "\n",
       "           noise  cat_A_scenario  cat_B_scenario  cat_C_scenario  \\\n",
       "0     100.000893              A1              B4              C1   \n",
       "1     100.000783              A1              B3              C0   \n",
       "13    100.000825              A1              B0              C1   \n",
       "16      0.000479              A1              B4              C4   \n",
       "17    100.000252              A1              B1              C3   \n",
       "...          ...             ...             ...             ...   \n",
       "2383    5.000552              A1              B4              C0   \n",
       "2385    5.000693              A1              B3              C2   \n",
       "2393    5.000722              A1              B4              C4   \n",
       "2398    5.000264              A1              B1              C4   \n",
       "2399    5.000156              A1              B1              C4   \n",
       "\n",
       "     cat_D_scenario  \n",
       "0                D0  \n",
       "1                D1  \n",
       "13               D4  \n",
       "16               D3  \n",
       "17               D4  \n",
       "...             ...  \n",
       "2383             D4  \n",
       "2385             D3  \n",
       "2393             D2  \n",
       "2398             D0  \n",
       "2399             D0  \n",
       "\n",
       "[513 rows x 23 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_A1 = df[df['cat_A_scenario']=='A1']\n",
    "df_A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.91035518762444\n",
      "[ -3.84235504 -13.45891789  -5.98115784  -1.00151876  -5.2084735\n",
      "  -0.95495595  -3.15645822  -7.2888877   -2.32297686  -3.7339574\n",
      "  -5.08132817   1.23153583]\n"
     ]
    }
   ],
   "source": [
    "X =df_A1[['B1', 'B2', 'B3', 'B4','C1', 'C2', 'C3', 'C4','D1', 'D2', 'D3', 'D4']]\n",
    "y= df_A1['noise']\n",
    "reg = LinearRegression().fit(X, y)\n",
    "reg.score(X, y)\n",
    "const = reg.intercept_\n",
    "coef = reg.coef_\n",
    "print(const)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch pad, not finished\n",
    "for i in cat_A:\n",
    "    df_i = df[df.cat_A_scenario == i]\n",
    "    X =df_i[df_i.columns[1:17]]\n",
    "    y= df_i['noise']\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    reg.score(X, y)\n",
    "    const = reg.intercept_\n",
    "    coef = reg.coef_\n",
    "    intercept.append(const)\n",
    "    coefficients.append(coef)\n",
    "#print(UID)    \n",
    "intercep_new = pd.DataFrame(intercept)\n",
    "coefficients_new = pd.DataFrame(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
